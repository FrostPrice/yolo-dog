{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46a22f67",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import all necessary libraries for YOLO training, computer vision, and data visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36d07551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3060 Ti\n",
      "CUDA Version: 12.8\n"
     ]
    }
   ],
   "source": [
    "# Core Libraries\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# YOLO and Deep Learning\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import urllib\n",
    "from pycocotools.coco import COCO\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Computer Vision\n",
    "import cv2\n",
    "\n",
    "# Data Science and Visualization\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Utilities\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check if GPU is available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "if device == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"Training on CPU will be significantly slower than GPU\")\n",
    "    print(\"Consider using Google Colab or a GPU-enabled environment\")\n",
    "\n",
    "# Set matplotlib style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b988d6",
   "metadata": {},
   "source": [
    "## 2. Setup Project Directories and Configuration\n",
    "\n",
    "Create necessary directories for custom dataset training and define training configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508dd494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Configuration:\n",
      "â”œâ”€â”€ Data Directory: /home/joni/Documentos/univali/inteligencia_artificial/yolo-dog/data\n",
      "â”œâ”€â”€ Images Directory: /home/joni/Documentos/univali/inteligencia_artificial/yolo-dog/data/images\n",
      "â”‚   â”œâ”€â”€ train: /home/joni/Documentos/univali/inteligencia_artificial/yolo-dog/data/images/train\n",
      "â”‚   â”œâ”€â”€ val: /home/joni/Documentos/univali/inteligencia_artificial/yolo-dog/data/images/val\n",
      "â”‚   â””â”€â”€ test: /home/joni/Documentos/univali/inteligencia_artificial/yolo-dog/data/images/test\n",
      "â”œâ”€â”€ Labels Directory: /home/joni/Documentos/univali/inteligencia_artificial/yolo-dog/data/labels\n",
      "â”‚   â”œâ”€â”€ train: /home/joni/Documentos/univali/inteligencia_artificial/yolo-dog/data/labels/train\n",
      "â”‚   â”œâ”€â”€ val: /home/joni/Documentos/univali/inteligencia_artificial/yolo-dog/data/labels/val\n",
      "â”‚   â””â”€â”€ test: /home/joni/Documentos/univali/inteligencia_artificial/yolo-dog/data/labels/test\n",
      "â”œâ”€â”€ Models Directory: /home/joni/Documentos/univali/inteligencia_artificial/yolo-dog/models\n",
      "â””â”€â”€ Outputs Directory: /home/joni/Documentos/univali/inteligencia_artificial/yolo-dog/outputs\n",
      "\n",
      "============================================================\n",
      "TRAINING CONFIGURATION\n",
      "============================================================\n",
      "\n",
      "ðŸ“¦ Model Settings:\n",
      "  Model:       yolov8n.pt\n",
      "  Image size:  640px\n",
      "  Batch size:  16\n",
      "  Epochs:      100\n",
      "  Patience:    20 (early stopping)\n",
      "\n",
      "ðŸŽ¯ Inference Settings:\n",
      "  Confidence:  0.25 (25% minimum)\n",
      "  IOU:         0.45 (NMS threshold)\n",
      "\n",
      "ðŸ“š Learning Settings:\n",
      "  Initial LR:  0.01\n",
      "  Final LR:    0.01\n",
      "  Momentum:    0.937\n",
      "  Weight decay: 0.0005\n",
      "\n",
      "ðŸ”„ Data Augmentation:\n",
      "  HSV:         H=0.015, S=0.7, V=0.4\n",
      "  Flip LR:     50%\n",
      "  Translate:   10%\n",
      "  Scale:       50%\n",
      "  Mosaic:      100%\n",
      "\n",
      "ðŸ’¡ TIP: These settings are optimized for single-class dog detection\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Project Configuration\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "IMAGES_DIR = DATA_DIR / 'images'\n",
    "LABELS_DIR = DATA_DIR / 'labels'\n",
    "MODELS_DIR = PROJECT_ROOT / 'models'\n",
    "OUTPUTS_DIR = PROJECT_ROOT / 'outputs'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for directory in [DATA_DIR, IMAGES_DIR, LABELS_DIR, MODELS_DIR, OUTPUTS_DIR]:\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create train/val/test subdirectories\n",
    "for split in ['train', 'val', 'test']:\n",
    "    (IMAGES_DIR / split).mkdir(exist_ok=True)\n",
    "    (LABELS_DIR / split).mkdir(exist_ok=True)\n",
    "\n",
    "# Configuration dictionary - Training settings\n",
    "config = {\n",
    "    'model_name': 'yolov8n.pt',  # YOLOv8 Nano - mais estÃ¡vel\n",
    "    'img_size': 640,\n",
    "    'batch_size': 16,\n",
    "    'epochs': 100,  # Aumentado para dar tempo ao modelo aprender\n",
    "    'patience': 20,  # Early stopping apÃ³s 20 Ã©pocas sem melhoria\n",
    "    'confidence_threshold': 0.25,  # Threshold padrÃ£o (25% confianÃ§a mÃ­nima)\n",
    "    'iou_threshold': 0.45,  # IOU padrÃ£o para NMS\n",
    "    'dog_class_id': 0,  # Custom dataset - single class (dog)\n",
    "    # HiperparÃ¢metros de treinamento otimizados\n",
    "    'lr0': 0.01,  # Learning rate inicial\n",
    "    'lrf': 0.01,  # Learning rate final (fraÃ§Ã£o de lr0)\n",
    "    'momentum': 0.937,  # SGD momentum\n",
    "    'weight_decay': 0.0005,  # L2 regularization\n",
    "    'warmup_epochs': 3,  # Ã‰pocas de warmup\n",
    "    'warmup_momentum': 0.8,  # Momentum inicial durante warmup\n",
    "    'warmup_bias_lr': 0.1,  # Learning rate de bias durante warmup\n",
    "    'box': 7.5,  # Box loss gain\n",
    "    'cls': 0.5,  # Class loss gain (baixo pois sÃ³ temos 1 classe)\n",
    "    'dfl': 1.5,  # DFL loss gain\n",
    "    'hsv_h': 0.015,  # Augmentation: Hue\n",
    "    'hsv_s': 0.7,  # Augmentation: Saturation\n",
    "    'hsv_v': 0.4,  # Augmentation: Value\n",
    "    'degrees': 0.0,  # Rotation (desabilitado para cachorros)\n",
    "    'translate': 0.1,  # Translation\n",
    "    'scale': 0.5,  # Scale\n",
    "    'shear': 0.0,  # Shear (desabilitado)\n",
    "    'perspective': 0.0,  # Perspective (desabilitado)\n",
    "    'flipud': 0.0,  # Flip vertical (desabilitado para cachorros)\n",
    "    'fliplr': 0.5,  # Flip horizontal (50% chance)\n",
    "    'mosaic': 1.0,  # Mosaic augmentation\n",
    "    'mixup': 0.0,  # Mixup augmentation (desabilitado para simplicidade)\n",
    "    'copy_paste': 0.0  # Copy-paste augmentation (desabilitado)\n",
    "}\n",
    "\n",
    "print(\"Project Configuration:\")\n",
    "print(f\"â”œâ”€â”€ Data Directory: {DATA_DIR}\")\n",
    "print(f\"â”œâ”€â”€ Images Directory: {IMAGES_DIR}\")\n",
    "print(f\"â”‚   â”œâ”€â”€ train: {IMAGES_DIR / 'train'}\")\n",
    "print(f\"â”‚   â”œâ”€â”€ val: {IMAGES_DIR / 'val'}\")\n",
    "print(f\"â”‚   â””â”€â”€ test: {IMAGES_DIR / 'test'}\")\n",
    "print(f\"â”œâ”€â”€ Labels Directory: {LABELS_DIR}\")\n",
    "print(f\"â”‚   â”œâ”€â”€ train: {LABELS_DIR / 'train'}\")\n",
    "print(f\"â”‚   â”œâ”€â”€ val: {LABELS_DIR / 'val'}\")\n",
    "print(f\"â”‚   â””â”€â”€ test: {LABELS_DIR / 'test'}\")\n",
    "print(f\"â”œâ”€â”€ Models Directory: {MODELS_DIR}\")\n",
    "print(f\"â””â”€â”€ Outputs Directory: {OUTPUTS_DIR}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TRAINING CONFIGURATION\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nModel Settings:\")\n",
    "print(f\"  Model:       {config['model_name']}\")\n",
    "print(f\"  Image size:  {config['img_size']}px\")\n",
    "print(f\"  Batch size:  {config['batch_size']}\")\n",
    "print(f\"  Epochs:      {config['epochs']}\")\n",
    "print(f\"  Patience:    {config['patience']} (early stopping)\")\n",
    "\n",
    "print(f\"\\nInference Settings:\")\n",
    "print(f\"  Confidence:  {config['confidence_threshold']} (25% minimum)\")\n",
    "print(f\"  IOU:         {config['iou_threshold']} (NMS threshold)\")\n",
    "\n",
    "print(f\"\\nLearning Settings:\")\n",
    "print(f\"  Initial LR:  {config['lr0']}\")\n",
    "print(f\"  Final LR:    {config['lrf']}\")\n",
    "print(f\"  Momentum:    {config['momentum']}\")\n",
    "print(f\"  Weight decay: {config['weight_decay']}\")\n",
    "\n",
    "print(f\"\\nData Augmentation:\")\n",
    "print(f\"  HSV:         H={config['hsv_h']}, S={config['hsv_s']}, V={config['hsv_v']}\")\n",
    "print(f\"  Flip LR:     {config['fliplr']*100:.0f}%\")\n",
    "print(f\"  Translate:   {config['translate']*100:.0f}%\")\n",
    "print(f\"  Scale:       {config['scale']*100:.0f}%\")\n",
    "print(f\"  Mosaic:      {config['mosaic']*100:.0f}%\")\n",
    "\n",
    "print(f\"\\nTIP: These settings are optimized for single-class dog detection\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0915164",
   "metadata": {},
   "source": [
    "## 2.5. Download and Format COCO Dataset for Training (Optional)\n",
    "\n",
    "Download COCO 2017 dataset, filter for dog images, and convert to YOLO format for training.\n",
    "\n",
    "**What this section does:**\n",
    "- Downloads COCO 2017 test dataset annotations\n",
    "- Filters images containing dogs (COCO class_id: 18)\n",
    "- Downloads filtered images\n",
    "- Converts COCO bbox format to YOLO format\n",
    "- Splits data into train (70%), val (20%), test (10%)\n",
    "- Organizes into proper directory structure\n",
    "\n",
    "**Note:** Skip this section if you already have a custom dataset prepared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb59773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for COCO download\n",
    "COCO_CONFIG = {\n",
    "    'annotations_url': 'http://images.cocodataset.org/annotations/annotations_trainval2017.zip',\n",
    "    'images_base_url': 'http://images.cocodataset.org/train2017/',\n",
    "    'dog_class_id': 18,  # Dog class in COCO dataset\n",
    "    'max_samples': None,  # Maximum images to download (set None for all dog images)\n",
    "    'train_split': 0.7, # Data to train the model\n",
    "    'val_split': 0.2, # Data to validate the model during training\n",
    "    'test_split': 0.1 # Data to test the model after training\n",
    "}\n",
    "\n",
    "def download_file(url, dest_path):\n",
    "    \"\"\"Download file with progress bar\"\"\"\n",
    "    print(f\"Downloading {url}...\")\n",
    "    try:\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            total_size = int(response.headers.get('content-length', 0))\n",
    "            with open(dest_path, 'wb') as f, tqdm(total=total_size, unit='B', unit_scale=True) as pbar:\n",
    "                while True:\n",
    "                    chunk = response.read(8192)\n",
    "                    if not chunk:\n",
    "                        break\n",
    "                    f.write(chunk)\n",
    "                    pbar.update(len(chunk))\n",
    "        print(f\"Downloaded to {dest_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading: {e}\")\n",
    "        return False\n",
    "\n",
    "def coco_to_yolo_bbox(bbox, img_width, img_height):\n",
    "    \"\"\"\n",
    "    Convert COCO bbox format to YOLO format\n",
    "    COCO: [x_min, y_min, width, height] (absolute pixels)\n",
    "    YOLO: [center_x, center_y, width, height] (normalized 0-1)\n",
    "    \"\"\"\n",
    "    x_min, y_min, width, height = bbox\n",
    "    \n",
    "    # Calculate center coordinates\n",
    "    center_x = (x_min + width / 2) / img_width\n",
    "    center_y = (y_min + height / 2) / img_height\n",
    "    \n",
    "    # Normalize width and height\n",
    "    norm_width = width / img_width\n",
    "    norm_height = height / img_height\n",
    "    \n",
    "    return [center_x, center_y, norm_width, norm_height]\n",
    "\n",
    "def download_and_format_coco_dogs():\n",
    "    \"\"\"Main function to download and format COCO dog dataset\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"COCO Dog Dataset Download and Formatting\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create temporary directory for annotations\n",
    "    temp_dir = PROJECT_ROOT / 'temp_coco'\n",
    "    temp_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Download annotations\n",
    "    annotations_zip = temp_dir / 'annotations_trainval2017.zip'\n",
    "    if not annotations_zip.exists():\n",
    "        if not download_file(COCO_CONFIG['annotations_url'], annotations_zip):\n",
    "            print(\"Failed to download annotations\")\n",
    "            return\n",
    "        \n",
    "        # Extract annotations\n",
    "        print(\"Extracting annotations...\")\n",
    "        shutil.unpack_archive(annotations_zip, temp_dir)\n",
    "    \n",
    "    # Load COCO annotations\n",
    "    annotations_file = temp_dir / 'annotations' / 'instances_train2017.json'\n",
    "    print(f\"\\nLoading COCO annotations from {annotations_file}\")\n",
    "    coco = COCO(str(annotations_file))\n",
    "    \n",
    "    # Get all images with dogs\n",
    "    dog_class_id = COCO_CONFIG['dog_class_id']\n",
    "    dog_img_ids = coco.getImgIds(catIds=[dog_class_id])\n",
    "    print(f\"Found {len(dog_img_ids)} images containing dogs in COCO val2017\")\n",
    "    \n",
    "    # Limit samples if specified\n",
    "    if COCO_CONFIG['max_samples'] and len(dog_img_ids) > COCO_CONFIG['max_samples']:\n",
    "        dog_img_ids = dog_img_ids[:COCO_CONFIG['max_samples']]\n",
    "        print(f\"Limiting to {COCO_CONFIG['max_samples']} images\")\n",
    "    \n",
    "    # Get random non-dog images for test set (negative examples)\n",
    "    all_img_ids = coco.getImgIds()\n",
    "    non_dog_img_ids = [img_id for img_id in all_img_ids if img_id not in dog_img_ids]\n",
    "    \n",
    "    # Sample random non-dog images (about 50% of test set will be non-dog images)\n",
    "    num_test_dogs = int(len(dog_img_ids) * COCO_CONFIG['test_split'])\n",
    "    num_non_dog_test = num_test_dogs  # Equal number = 50% each\n",
    "    random.seed(42)\n",
    "    non_dog_test_ids = random.sample(non_dog_img_ids, min(num_non_dog_test, len(non_dog_img_ids)))\n",
    "    print(f\"Selected {len(non_dog_test_ids)} random non-dog images for test set (50% of test data)\")\n",
    "    \n",
    "    # Split into train/val/test\n",
    "    train_ratio = COCO_CONFIG['train_split']\n",
    "    val_ratio = COCO_CONFIG['val_split']\n",
    "    test_ratio = COCO_CONFIG['test_split']\n",
    "    \n",
    "    # First split: separate test set (only from dog images)\n",
    "    train_val_ids, test_dog_ids = train_test_split(\n",
    "        dog_img_ids, \n",
    "        test_size=test_ratio, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Add non-dog images to test set\n",
    "    test_ids = test_dog_ids + non_dog_test_ids\n",
    "    \n",
    "    # Second split: separate train and val\n",
    "    val_size = val_ratio / (train_ratio + val_ratio)\n",
    "    train_ids, val_ids = train_test_split(\n",
    "        train_val_ids, \n",
    "        test_size=val_size, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    splits = {\n",
    "        'train': train_ids,\n",
    "        'val': val_ids,\n",
    "        'test': test_ids\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nDataset splits:\")\n",
    "    print(f\"  Train: {len(train_ids)} images ({train_ratio*100:.0f}%) - all with dogs\")\n",
    "    print(f\"  Val:   {len(val_ids)} images ({val_ratio*100:.0f}%) - all with dogs\")\n",
    "    print(f\"  Test:  {len(test_ids)} images ({len(test_dog_ids)} dogs + {len(non_dog_test_ids)} non-dogs)\")\n",
    "    \n",
    "    # Process each split\n",
    "    for split_name, img_ids in splits.items():\n",
    "        print(f\"\\nProcessing {split_name} set...\")\n",
    "        images_split_dir = IMAGES_DIR / split_name\n",
    "        labels_split_dir = LABELS_DIR / split_name\n",
    "        \n",
    "        # Clear existing files in split\n",
    "        for f in images_split_dir.glob('*'):\n",
    "            f.unlink()\n",
    "        for f in labels_split_dir.glob('*.txt'):\n",
    "            f.unlink()\n",
    "        \n",
    "        for img_id in tqdm(img_ids, desc=f\"Downloading {split_name}\"):\n",
    "            # Get image info\n",
    "            img_info = coco.loadImgs(img_id)[0]\n",
    "            img_filename = img_info['file_name']\n",
    "            img_width = img_info['width']\n",
    "            img_height = img_info['height']\n",
    "            \n",
    "            # Download image\n",
    "            img_url = COCO_CONFIG['images_base_url'] + img_filename\n",
    "            img_path = images_split_dir / img_filename\n",
    "            \n",
    "            try:\n",
    "                urllib.request.urlretrieve(img_url, img_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to download {img_filename}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Get annotations for this image (only dogs)\n",
    "            ann_ids = coco.getAnnIds(imgIds=img_id, catIds=[dog_class_id])\n",
    "            annotations = coco.loadAnns(ann_ids)\n",
    "            \n",
    "            # Convert to YOLO format and save\n",
    "            label_path = labels_split_dir / img_filename.replace('.jpg', '.txt')\n",
    "            \n",
    "            # For non-dog images (negative examples), create empty label file\n",
    "            if len(annotations) == 0:\n",
    "                # Create empty label file for images with no dogs\n",
    "                label_path.touch()\n",
    "            else:\n",
    "                # Write dog annotations\n",
    "                with open(label_path, 'w') as f:\n",
    "                    for ann in annotations:\n",
    "                        bbox = ann['bbox']\n",
    "                        yolo_bbox = coco_to_yolo_bbox(bbox, img_width, img_height)\n",
    "                        # YOLO format: class_id center_x center_y width height\n",
    "                        f.write(f\"0 {yolo_bbox[0]:.6f} {yolo_bbox[1]:.6f} {yolo_bbox[2]:.6f} {yolo_bbox[3]:.6f}\\n\")\n",
    "    \n",
    "    # Clean up temporary files\n",
    "    print(\"\\nCleaning up temporary files...\")\n",
    "    shutil.rmtree(temp_dir)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COCO Dataset Download Complete!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Images saved to: {IMAGES_DIR}\")\n",
    "    print(f\"Labels saved to: {LABELS_DIR}\")\n",
    "    print(\"\\nDataset is ready for training!\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Option to download COCO dataset\n",
    "print(\"COCO Dog Dataset Preparation\")\n",
    "print(\"-\" * 60)\n",
    "print(\"This will download COCO 2017 validation images containing dogs\")\n",
    "print(f\"and format them for YOLO training.\\n\")\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  - Max samples: {COCO_CONFIG['max_samples']}\")\n",
    "print(f\"  - Train/Val/Test split: {COCO_CONFIG['train_split']}/{COCO_CONFIG['val_split']}/{COCO_CONFIG['test_split']}\")\n",
    "print(f\"  - Target directory: {DATA_DIR}\")\n",
    "print(\"\\nTo download and format COCO dataset, run:\")\n",
    "print(\"  download_and_format_coco_dogs()\")\n",
    "print(\"\\nOr skip this cell if you have your own dataset ready.\")\n",
    "\n",
    "# Uncomment the line below to automatically download COCO dataset\n",
    "download_and_format_coco_dogs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc5c9fe",
   "metadata": {},
   "source": [
    "## 3. Define Helper Functions\n",
    "\n",
    "Create utility functions for image processing, visualization, training metrics, and dog detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0508ffba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined successfully!\n"
     ]
    }
   ],
   "source": [
    "def visualize_detections(image_path, results, save_path=None):\n",
    "    \"\"\"Visualize detection results with bounding boxes\"\"\"\n",
    "    # Read image\n",
    "    img = cv2.imread(str(image_path))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Plot results\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "    ax.imshow(img)\n",
    "    \n",
    "    # Draw bounding boxes\n",
    "    if results and len(results[0].boxes) > 0:\n",
    "        for box in results[0].boxes:\n",
    "            # Get box coordinates\n",
    "            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "            confidence = box.conf[0].cpu().numpy()\n",
    "            class_id = int(box.cls[0].cpu().numpy())\n",
    "            \n",
    "            # Draw rectangle\n",
    "            rect = plt.Rectangle((x1, y1), x2-x1, y2-y1, \n",
    "                                fill=False, edgecolor='red', linewidth=2)\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "            # Add label\n",
    "            label = f\"Dog {confidence:.2f}\"\n",
    "            ax.text(x1, y1-10, label, color='red', fontsize=12,\n",
    "                   bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight', dpi=150)\n",
    "        print(f\"Saved visualization to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "def process_video_detections(video_path, model, output_path, conf_threshold=0.25):\n",
    "    \"\"\"Process video file and detect dogs frame by frame\"\"\"\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    \n",
    "    # Get video properties\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # Define codec and create VideoWriter\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(str(output_path), fourcc, fps, (width, height))\n",
    "    \n",
    "    print(f\"Processing video: {total_frames} frames at {fps} FPS\")\n",
    "    \n",
    "    frame_count = 0\n",
    "    with tqdm(total=total_frames) as pbar:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # Run detection\n",
    "            results = model(frame, conf=conf_threshold, verbose=False)\n",
    "            \n",
    "            # Draw results on frame\n",
    "            annotated_frame = results[0].plot()\n",
    "            \n",
    "            # Write frame\n",
    "            out.write(annotated_frame)\n",
    "            \n",
    "            frame_count += 1\n",
    "            pbar.update(1)\n",
    "    \n",
    "    cap.release()\n",
    "    out.release()\n",
    "    \n",
    "    print(f\"Processed {frame_count} frames. Output saved to {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "def display_metrics(results_df):\n",
    "    \"\"\"Display training metrics and create visualizations\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Plot losses\n",
    "    if 'train/box_loss' in results_df.columns:\n",
    "        axes[0, 0].plot(results_df['epoch'], results_df['train/box_loss'], label='Box Loss')\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].set_title('Training Box Loss')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True)\n",
    "    \n",
    "    # Plot mAP\n",
    "    if 'metrics/mAP50(B)' in results_df.columns:\n",
    "        axes[0, 1].plot(results_df['epoch'], results_df['metrics/mAP50(B)'], label='mAP@0.5')\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('mAP')\n",
    "        axes[0, 1].set_title('Mean Average Precision')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True)\n",
    "    \n",
    "    # Plot precision and recall\n",
    "    if 'metrics/precision(B)' in results_df.columns:\n",
    "        axes[1, 0].plot(results_df['epoch'], results_df['metrics/precision(B)'], label='Precision')\n",
    "        axes[1, 0].plot(results_df['epoch'], results_df['metrics/recall(B)'], label='Recall')\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('Score')\n",
    "        axes[1, 0].set_title('Precision and Recall')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True)\n",
    "    \n",
    "    # Summary statistics\n",
    "    axes[1, 1].axis('off')\n",
    "    if not results_df.empty:\n",
    "        summary_text = f\"Training Summary\\n\\n\"\n",
    "        summary_text += f\"Total Epochs: {len(results_df)}\\n\"\n",
    "        if 'metrics/mAP50(B)' in results_df.columns:\n",
    "            summary_text += f\"Best mAP@0.5: {results_df['metrics/mAP50(B)'].max():.4f}\\n\"\n",
    "        if 'metrics/precision(B)' in results_df.columns:\n",
    "            summary_text += f\"Best Precision: {results_df['metrics/precision(B)'].max():.4f}\\n\"\n",
    "        if 'metrics/recall(B)' in results_df.columns:\n",
    "            summary_text += f\"Best Recall: {results_df['metrics/recall(B)'].max():.4f}\\n\"\n",
    "        \n",
    "        axes[1, 1].text(0.1, 0.5, summary_text, fontsize=14, family='monospace',\n",
    "                       verticalalignment='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "print(\"Helper functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae767e6",
   "metadata": {},
   "source": [
    "## 4. Load Base YOLO Model\n",
    "\n",
    "Initialize YOLO model with pre-trained COCO weights as a starting point for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fd7ffe8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading yolo11n.pt model as base...\n",
      "\n",
      "Base model loaded successfully!\n",
      "Model type: <class 'ultralytics.models.yolo.model.YOLO'>\n",
      "Device: cpu\n",
      "\n",
      "This model will be fine-tuned on your custom dog dataset.\n",
      "The pre-trained weights provide a good starting point for training.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained YOLO model (will be fine-tuned)\n",
    "print(f\"Loading {config['model_name']} model as base...\")\n",
    "model = YOLO(config['model_name'])\n",
    "\n",
    "# Display model information\n",
    "print(f\"\\nBase model loaded successfully!\")\n",
    "print(f\"Model type: {type(model)}\")\n",
    "print(f\"Device: {model.device}\")\n",
    "print(\"\\nThis model will be fine-tuned on your custom dog dataset.\")\n",
    "print(\"The pre-trained weights provide a good starting point for training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c394c46",
   "metadata": {},
   "source": [
    "## 5. Prepare Custom Dataset for Training\n",
    "\n",
    "Set up the dataset configuration file and verify dataset structure.\n",
    "\n",
    "**Required**: Your dataset must follow YOLO format:\n",
    "- Images in `data/images/train/`, `data/images/val/`, `data/images/test/`\n",
    "- Labels in `data/labels/train/`, `data/labels/val/`, `data/labels/test/`\n",
    "- Each image has a corresponding `.txt` label file\n",
    "- Label format: `class_id center_x center_y width height` (all normalized 0-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7c394c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset configuration saved to: /home/joni/Documentos/univali/inteligencia_artificial/yolo-dog/data/dataset.yaml\n",
      "\n",
      "Dataset Configuration:\n",
      "names:\n",
      "  0: dog\n",
      "nc: 1\n",
      "path: /home/joni/Documentos/univali/inteligencia_artificial/yolo-dog/data\n",
      "test: images/test\n",
      "train: images/train\n",
      "val: images/val\n",
      "\n",
      "\n",
      "Verifying dataset structure...\n",
      "\n",
      "Dataset Summary:\n",
      "â”œâ”€â”€ Training set:\n",
      "â”‚   â”œâ”€â”€ Images: 349\n",
      "â”‚   â””â”€â”€ Labels: 349\n",
      "â”œâ”€â”€ Validation set:\n",
      "â”‚   â”œâ”€â”€ Images: 101\n",
      "â”‚   â””â”€â”€ Labels: 101\n",
      "â””â”€â”€ Test set:\n",
      "    â”œâ”€â”€ Images: 100\n",
      "    â””â”€â”€ Labels: 100\n",
      "\n",
      "Dataset is ready for training!\n"
     ]
    }
   ],
   "source": [
    "# Create dataset configuration file for training\n",
    "dataset_config = {\n",
    "    'path': str(DATA_DIR.absolute()),\n",
    "    'train': 'images/train',\n",
    "    'val': 'images/val',\n",
    "    'test': 'images/test',\n",
    "    'names': {\n",
    "        0: 'dog'\n",
    "    },\n",
    "    'nc': 1  # number of classes\n",
    "}\n",
    "\n",
    "# Save dataset configuration\n",
    "dataset_yaml_path = DATA_DIR / 'dataset.yaml'\n",
    "with open(dataset_yaml_path, 'w') as f:\n",
    "    yaml.dump(dataset_config, f, default_flow_style=False)\n",
    "\n",
    "print(f\"Dataset configuration saved to: {dataset_yaml_path}\")\n",
    "print(\"\\nDataset Configuration:\")\n",
    "print(yaml.dump(dataset_config, default_flow_style=False))\n",
    "\n",
    "# Verify dataset structure\n",
    "print(\"\\nVerifying dataset structure...\")\n",
    "train_images = list((IMAGES_DIR / 'train').glob('*.*'))\n",
    "train_labels = list((LABELS_DIR / 'train').glob('*.txt'))\n",
    "val_images = list((IMAGES_DIR / 'val').glob('*.*'))\n",
    "val_labels = list((LABELS_DIR / 'val').glob('*.txt'))\n",
    "test_images = list((IMAGES_DIR / 'test').glob('*.*'))\n",
    "test_labels = list((LABELS_DIR / 'test').glob('*.txt'))\n",
    "\n",
    "print(f\"\\nDataset Summary:\")\n",
    "print(f\"â”œâ”€â”€ Training set:\")\n",
    "print(f\"â”‚   â”œâ”€â”€ Images: {len(train_images)}\")\n",
    "print(f\"â”‚   â””â”€â”€ Labels: {len(train_labels)}\")\n",
    "print(f\"â”œâ”€â”€ Validation set:\")\n",
    "print(f\"â”‚   â”œâ”€â”€ Images: {len(val_images)}\")\n",
    "print(f\"â”‚   â””â”€â”€ Labels: {len(val_labels)}\")\n",
    "print(f\"â””â”€â”€ Test set:\")\n",
    "print(f\"    â”œâ”€â”€ Images: {len(test_images)}\")\n",
    "print(f\"    â””â”€â”€ Labels: {len(test_labels)}\")\n",
    "\n",
    "# Check if dataset is ready\n",
    "if len(train_images) > 0 and len(val_images) > 0:\n",
    "    print(f\"\\nDataset is ready for training!\")\n",
    "    if len(train_labels) != len(train_images):\n",
    "        print(f\"Warning: Number of training labels ({len(train_labels)}) doesn't match images ({len(train_images)})\")\n",
    "    if len(val_labels) != len(val_images):\n",
    "        print(f\"Warning: Number of validation labels ({len(val_labels)}) doesn't match images ({len(val_images)})\")\n",
    "else:\n",
    "    print(f\"\\nDataset not ready!\")\n",
    "    print(f\"   Please add labeled images to:\")\n",
    "    print(f\"   - Training: {IMAGES_DIR / 'train'} and {LABELS_DIR / 'train'}\")\n",
    "    print(f\"   - Validation: {IMAGES_DIR / 'val'} and {LABELS_DIR / 'val'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5705efc6",
   "metadata": {},
   "source": [
    "## 5.5. Dataset Quality Check\n",
    "\n",
    "Analyze dataset quality before training to identify potential issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109fbe01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATASET QUALITY ANALYSIS\n",
      "============================================================\n",
      "\n",
      "ðŸ“‚ TRAIN Split:\n",
      "  Total images:  349\n",
      "  Total labels:  349\n",
      "  Images with objects: 349 (100.0%)\n",
      "  Empty labels (negatives): 0 (0.0%)\n",
      "  Total objects detected: 473\n",
      "  Avg objects/image: 1.36\n",
      "  Max objects/image: 20\n",
      "\n",
      "ðŸ“‚ VAL Split:\n",
      "  Total images:  101\n",
      "  Total labels:  101\n",
      "  Images with objects: 101 (100.0%)\n",
      "  Empty labels (negatives): 0 (0.0%)\n",
      "  Total objects detected: 117\n",
      "  Avg objects/image: 1.16\n",
      "  Max objects/image: 4\n",
      "\n",
      "ðŸ“‚ TEST Split:\n",
      "  Total images:  100\n",
      "  Total labels:  100\n",
      "  Images with objects: 51 (51.0%)\n",
      "  Empty labels (negatives): 49 (49.0%)\n",
      "  Total objects detected: 59\n",
      "  Avg objects/image: 1.16\n",
      "  Max objects/image: 4\n",
      "\n",
      "============================================================\n",
      "ðŸ“‹ RECOMMENDATIONS\n",
      "============================================================\n",
      "\n",
      "âœ… Test set contains 49.0% negative samples\n",
      "\n",
      "âœ… All bounding boxes are properly formatted\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def analyze_dataset_quality(split_name):\n",
    "    \"\"\"Analyze label quality for a dataset split\"\"\"\n",
    "    labels_dir = LABELS_DIR / split_name\n",
    "    images_dir = IMAGES_DIR / split_name\n",
    "    \n",
    "    label_files = list(labels_dir.glob('*.txt'))\n",
    "    image_files = list(images_dir.glob('*.*'))\n",
    "    \n",
    "    stats = {\n",
    "        'total_labels': len(label_files),\n",
    "        'total_images': len(image_files),\n",
    "        'empty_labels': 0,\n",
    "        'labels_with_objects': 0,\n",
    "        'total_objects': 0,\n",
    "        'objects_per_image': [],\n",
    "        'bbox_issues': 0\n",
    "    }\n",
    "    \n",
    "    for label_file in label_files:\n",
    "        with open(label_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        if len(lines) == 0:\n",
    "            stats['empty_labels'] += 1\n",
    "        else:\n",
    "            stats['labels_with_objects'] += 1\n",
    "            stats['total_objects'] += len(lines)\n",
    "            stats['objects_per_image'].append(len(lines))\n",
    "            \n",
    "            # Check bbox format\n",
    "            for line in lines:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) != 5:\n",
    "                    stats['bbox_issues'] += 1\n",
    "                    continue\n",
    "                try:\n",
    "                    class_id, cx, cy, w, h = map(float, parts)\n",
    "                    # Check if normalized (0-1)\n",
    "                    if not (0 <= cx <= 1 and 0 <= cy <= 1 and 0 <= w <= 1 and 0 <= h <= 1):\n",
    "                        stats['bbox_issues'] += 1\n",
    "                except:\n",
    "                    stats['bbox_issues'] += 1\n",
    "    \n",
    "    return stats\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DATASET QUALITY ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for split in ['train', 'val', 'test']:\n",
    "    stats = analyze_dataset_quality(split)\n",
    "    \n",
    "    print(f\"\\n{split.upper()} Split:\")\n",
    "    print(f\"  Total images:  {stats['total_images']}\")\n",
    "    print(f\"  Total labels:  {stats['total_labels']}\")\n",
    "    print(f\"  Images with objects: {stats['labels_with_objects']} ({stats['labels_with_objects']/stats['total_labels']*100:.1f}%)\")\n",
    "    print(f\"  Empty labels (negatives): {stats['empty_labels']} ({stats['empty_labels']/stats['total_labels']*100:.1f}%)\")\n",
    "    print(f\"  Total objects detected: {stats['total_objects']}\")\n",
    "    \n",
    "    if stats['objects_per_image']:\n",
    "        avg_objects = sum(stats['objects_per_image']) / len(stats['objects_per_image'])\n",
    "        max_objects = max(stats['objects_per_image'])\n",
    "        print(f\"  Avg objects/image: {avg_objects:.2f}\")\n",
    "        print(f\"  Max objects/image: {max_objects}\")\n",
    "    \n",
    "    if stats['bbox_issues'] > 0:\n",
    "        print(f\"  Bbox format issues: {stats['bbox_issues']}\")\n",
    "\n",
    "# Recommendations\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"RECOMMENDATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "train_stats = analyze_dataset_quality('train')\n",
    "val_stats = analyze_dataset_quality('val')\n",
    "test_stats = analyze_dataset_quality('test')\n",
    "\n",
    "# Check for negative samples in training\n",
    "if train_stats['empty_labels'] > 0:\n",
    "    neg_ratio = train_stats['empty_labels'] / train_stats['total_labels'] * 100\n",
    "    print(f\"\\nTraining set contains {neg_ratio:.1f}% negative samples (no objects)\")\n",
    "    if neg_ratio > 10:\n",
    "        print(f\"   Recommendation: Remove or reduce negative samples in training\")\n",
    "        print(f\"   Negative samples are better suited for validation/test sets\")\n",
    "\n",
    "# Check for negative samples in test\n",
    "if test_stats['empty_labels'] > 0:\n",
    "    neg_ratio = test_stats['empty_labels'] / test_stats['total_labels'] * 100\n",
    "    print(f\"\\nTest set contains {neg_ratio:.1f}% negative samples\")\n",
    "    if neg_ratio > 60:\n",
    "        print(f\"   Too many negatives! This will skew validation metrics\")\n",
    "        print(f\"   Recommendation: Aim for 10-20% negative samples in test set\")\n",
    "    elif neg_ratio < 5:\n",
    "        print(f\"   Too few negatives! Add more negative samples for robust validation\")\n",
    "\n",
    "# Check dataset size\n",
    "if train_stats['total_images'] < 100:\n",
    "    print(f\"\\nSmall training set ({train_stats['total_images']} images)\")\n",
    "    print(f\"   Recommendation: Collect more training data (aim for 500+ images)\")\n",
    "    print(f\"   Consider using data augmentation to increase effective dataset size\")\n",
    "\n",
    "if train_stats['total_objects'] < 200:\n",
    "    print(f\"\\nFew training objects ({train_stats['total_objects']} total)\")\n",
    "    print(f\"   Recommendation: Increase training data for better model performance\")\n",
    "\n",
    "# Check bbox issues\n",
    "total_bbox_issues = train_stats['bbox_issues'] + val_stats['bbox_issues'] + test_stats['bbox_issues']\n",
    "if total_bbox_issues > 0:\n",
    "    print(f\"\\nBbox format issues detected: {total_bbox_issues} total\")\n",
    "    print(f\"   Action required: Fix label format before training!\")\n",
    "else:\n",
    "    print(f\"\\nAll bounding boxes are properly formatted\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db991b0",
   "metadata": {},
   "source": [
    "## 6. Train Custom Model\n",
    "\n",
    "Fine-tune YOLOv8 on your custom dog dataset with optimized hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7a9fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images found: 349\n",
      "Validation images found: 101\n",
      "\n",
      "============================================================\n",
      "STARTING MODEL TRAINING\n",
      "============================================================\n",
      "\n",
      "ðŸ“Š Dataset:\n",
      "  Training:   349 images\n",
      "  Validation: 101 images\n",
      "\n",
      "âš™ï¸  Training Configuration:\n",
      "  Device:     cuda\n",
      "  Model:      yolov8n.pt\n",
      "  Epochs:     100\n",
      "  Patience:   20 (early stopping)\n",
      "  Batch size: 16\n",
      "  Image size: 640px\n",
      "  Initial LR: 0.01\n",
      "\n",
      "â±ï¸  Estimated time:\n",
      "  ~2-4 hours on GPU for 349 images\n",
      "\n",
      "============================================================\n",
      "Training in progress...\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "âœ… TRAINING COMPLETED!\n",
      "============================================================\n",
      "\n",
      "ðŸ“ Model saved to: /home/joni/Documentos/univali/inteligencia_artificial/yolo-dog/models/yolo_dog_custom\n",
      "  â”œâ”€â”€ Best weights:  weights/best.pt\n",
      "  â”œâ”€â”€ Last weights:  weights/last.pt\n",
      "  â”œâ”€â”€ Metrics:       results.csv\n",
      "  â””â”€â”€ Plots:         *.png\n",
      "\n",
      "âœ… Loaded best model for inference: best.pt\n",
      "\n",
      "ðŸ“Š Final Training Metrics:\n",
      "  Epochs trained:  100\n",
      "  Best mAP@0.5:    0.7037\n",
      "  Best Precision:  0.8625\n",
      "  Best Recall:     0.7238\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Check if training data exists\n",
    "train_images = list((IMAGES_DIR / 'train').glob('*.*'))\n",
    "val_images = list((IMAGES_DIR / 'val').glob('*.*'))\n",
    "\n",
    "print(f\"Training images found: {len(train_images)}\")\n",
    "print(f\"Validation images found: {len(val_images)}\")\n",
    "\n",
    "if len(train_images) > 0 and len(val_images) > 0:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STARTING MODEL TRAINING\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nDataset:\")\n",
    "    print(f\"  Training:   {len(train_images)} images\")\n",
    "    print(f\"  Validation: {len(val_images)} images\")\n",
    "    \n",
    "    print(f\"\\nTraining Configuration:\")\n",
    "    print(f\"  Device:     {device}\")\n",
    "    print(f\"  Model:      {config['model_name']}\")\n",
    "    print(f\"  Epochs:     {config['epochs']}\")\n",
    "    print(f\"  Patience:   {config['patience']} (early stopping)\")\n",
    "    print(f\"  Batch size: {config['batch_size']}\")\n",
    "    print(f\"  Image size: {config['img_size']}px\")\n",
    "    print(f\"  Initial LR: {config['lr0']}\")\n",
    "    \n",
    "    print(f\"\\nEstimated time:\")\n",
    "    if device == 'cuda':\n",
    "        print(f\"  ~2-4 hours on GPU for {len(train_images)} images\")\n",
    "    else:\n",
    "        print(f\"  ~8-12 hours on CPU for {len(train_images)} images\")\n",
    "        print(f\"  Consider using GPU for faster training!\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Training in progress...\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    # Start training with optimized hyperparameters\n",
    "    results = model.train(\n",
    "        # Dataset\n",
    "        data=str(dataset_yaml_path),\n",
    "        \n",
    "        # Training duration\n",
    "        epochs=config['epochs'],\n",
    "        patience=config['patience'],\n",
    "        \n",
    "        # Model & hardware\n",
    "        imgsz=config['img_size'],\n",
    "        batch=config['batch_size'],\n",
    "        device=device,\n",
    "        \n",
    "        # Output\n",
    "        name='yolo_dog_custom',\n",
    "        project=str(MODELS_DIR),\n",
    "        save=True,\n",
    "        save_period=-1,  # Save checkpoint every N epochs (-1 = only save last)\n",
    "        \n",
    "        # Visualization\n",
    "        plots=True,\n",
    "        verbose=True,\n",
    "        \n",
    "        # Optimization\n",
    "        optimizer='auto',  # AdamW ou SGD automÃ¡tico\n",
    "        lr0=config['lr0'],\n",
    "        lrf=config['lrf'],\n",
    "        momentum=config['momentum'],\n",
    "        weight_decay=config['weight_decay'],\n",
    "        warmup_epochs=config['warmup_epochs'],\n",
    "        warmup_momentum=config['warmup_momentum'],\n",
    "        warmup_bias_lr=config['warmup_bias_lr'],\n",
    "        \n",
    "        # Loss gains\n",
    "        box=config['box'],\n",
    "        cls=config['cls'],\n",
    "        dfl=config['dfl'],\n",
    "        \n",
    "        # Data augmentation\n",
    "        hsv_h=config['hsv_h'],\n",
    "        hsv_s=config['hsv_s'],\n",
    "        hsv_v=config['hsv_v'],\n",
    "        degrees=config['degrees'],\n",
    "        translate=config['translate'],\n",
    "        scale=config['scale'],\n",
    "        shear=config['shear'],\n",
    "        perspective=config['perspective'],\n",
    "        flipud=config['flipud'],\n",
    "        fliplr=config['fliplr'],\n",
    "        mosaic=config['mosaic'],\n",
    "        mixup=config['mixup'],\n",
    "        copy_paste=config['copy_paste'],\n",
    "        \n",
    "        # Validation\n",
    "        val=True,\n",
    "        fraction=1.0,  # Usar 100% dos dados\n",
    "        \n",
    "        # Performance\n",
    "        cache=False,  # NÃ£o cachear imagens em memÃ³ria (economiza RAM)\n",
    "        workers=8,  # NÃºmero de workers para DataLoader\n",
    "        \n",
    "        # Other\n",
    "        seed=42,  # Reprodutibilidade\n",
    "        deterministic=True,  # Resultados determinÃ­sticos\n",
    "        single_cls=True,  # OtimizaÃ§Ã£o para single-class\n",
    "        rect=False,  # Rectangular training (desabilitado)\n",
    "        cos_lr=False,  # Cosine LR scheduler (desabilitado)\n",
    "        close_mosaic=10,  # Desabilitar mosaic nos Ãºltimos N epochs\n",
    "        resume=False,  # NÃ£o retomar treinamento anterior\n",
    "        amp=True,  # Automatic Mixed Precision (mais rÃ¡pido em GPU)\n",
    "        overlap_mask=True,\n",
    "        mask_ratio=4,\n",
    "        dropout=0.0,\n",
    "        nbs=64  # Nominal batch size\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRAINING COMPLETED!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nModel saved to: {MODELS_DIR / 'yolo_dog_custom'}\")\n",
    "    print(f\"  â”œâ”€â”€ Best weights:  weights/best.pt\")\n",
    "    print(f\"  â”œâ”€â”€ Last weights:  weights/last.pt\")\n",
    "    print(f\"  â”œâ”€â”€ Metrics:       results.csv\")\n",
    "    print(f\"  â””â”€â”€ Plots:         *.png\")\n",
    "    \n",
    "    # Load best model for inference\n",
    "    best_model_path = MODELS_DIR / 'yolo_dog_custom' / 'weights' / 'best.pt'\n",
    "    if best_model_path.exists():\n",
    "        model = YOLO(str(best_model_path))\n",
    "        print(f\"\\nLoaded best model for inference: {best_model_path.name}\")\n",
    "        \n",
    "        # Show final metrics\n",
    "        results_csv = MODELS_DIR / 'yolo_dog_custom' / 'results.csv'\n",
    "        if results_csv.exists():\n",
    "            df = pd.read_csv(results_csv)\n",
    "            print(f\"\\nFinal Training Metrics:\")\n",
    "            print(f\"  Epochs trained:  {len(df)}\")\n",
    "            print(f\"  Best mAP@0.5:    {df['metrics/mAP50(B)'].max():.4f}\")\n",
    "            print(f\"  Best Precision:  {df['metrics/precision(B)'].max():.4f}\")\n",
    "            print(f\"  Best Recall:     {df['metrics/recall(B)'].max():.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    \n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"NO TRAINING DATA FOUND\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nPlease add labeled images to train the model:\")\n",
    "    print(f\"\\nRequired directories:\")\n",
    "    print(f\"  Training images:  {IMAGES_DIR / 'train'}\")\n",
    "    print(f\"  Training labels:  {LABELS_DIR / 'train'}\")\n",
    "    print(f\"  Validation images: {IMAGES_DIR / 'val'}\")\n",
    "    print(f\"  Validation labels: {LABELS_DIR / 'val'}\")\n",
    "    \n",
    "    print(f\"\\nLabel Format (YOLO):\")\n",
    "    print(f\"  Each .txt file should contain:\")\n",
    "    print(f\"  class_id center_x center_y width height\")\n",
    "    print(f\"\\n  Example:\")\n",
    "    print(f\"  0 0.5 0.5 0.3 0.4\")\n",
    "    print(f\"\\n  (All values normalized 0-1 relative to image dimensions)\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a23f7a6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Model execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1349e928",
   "metadata": {},
   "source": [
    "## 7. Video Processing with Trained Model\n",
    "\n",
    "Process video files using your trained custom model to detect dogs in real-time video streams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad274cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for video files\n",
    "video_extensions = ['*.mp4', '*.avi', '*.mov', '*.mkv']\n",
    "video_files = []\n",
    "for ext in video_extensions:\n",
    "    video_files.extend(IMAGES_DIR.glob(ext))\n",
    "\n",
    "print(f\"Video files found: {len(video_files)}\")\n",
    "for video in video_files:\n",
    "    print(f\"  - {video.name}\")\n",
    "\n",
    "if video_files:\n",
    "    for video_path in video_files:\n",
    "        print(f\"\\nProcessing video: {video_path.name}\")\n",
    "        output_path = OUTPUTS_DIR / f\"detected_custom_{video_path.name}\"\n",
    "        \n",
    "        process_video_detections(\n",
    "            video_path=video_path,\n",
    "            model=model,\n",
    "            output_path=output_path,\n",
    "            conf_threshold=config['confidence_threshold']\n",
    "        )\n",
    "        \n",
    "        print(f\"Output video saved to: {output_path}\")\n",
    "else:\n",
    "    print(\"\\nNo video files found.\")\n",
    "    print(f\"Add video files to: {IMAGES_DIR}\")\n",
    "    print(\"\\nSupported formats: MP4, AVI, MOV, MKV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bdac2d",
   "metadata": {},
   "source": [
    "## 8. Batch Prediction on Custom Images\n",
    "\n",
    "Run inference on multiple images and save results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e8ffdf39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running batch prediction on validation images...\n",
      "\n",
      "Processing 2 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 31.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch Prediction Results:\n",
      "           image  dogs_detected  confidence_avg\n",
      "000000106525.jpg              0               0\n",
      "000000106850.jpg              0               0\n",
      "\n",
      "Summary:\n",
      "  Total images processed: 2\n",
      "  Images with dogs detected: 0\n",
      "  Total dogs detected: 0\n",
      "  Average confidence: 0.000\n",
      "\n",
      "Results saved to: /home/joni/Documentos/univali/inteligencia_artificial/yolo-dog/outputs/detection_results_custom.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Batch prediction on all images\n",
    "def batch_predict(image_dir, output_dir, model, conf_threshold=0.25):\n",
    "    \"\"\"Run prediction on all images in a directory\"\"\"\n",
    "    image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp']\n",
    "    all_images = []\n",
    "    for ext in image_extensions:\n",
    "        all_images.extend(Path(image_dir).glob(ext))\n",
    "    \n",
    "    if not all_images:\n",
    "        print(f\"No images found in {image_dir}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Processing {len(all_images)} images...\")\n",
    "    results_data = []\n",
    "    \n",
    "    for img_path in tqdm(all_images):\n",
    "        # Run inference\n",
    "        results = model(\n",
    "            str(img_path),\n",
    "            conf=conf_threshold,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Count detections\n",
    "        num_dogs = len(results[0].boxes)\n",
    "        \n",
    "        # Save results\n",
    "        results_data.append({\n",
    "            'image': img_path.name,\n",
    "            'dogs_detected': num_dogs,\n",
    "            'confidence_avg': results[0].boxes.conf.mean().item() if num_dogs > 0 else 0\n",
    "        })\n",
    "        \n",
    "        # Save annotated image\n",
    "        if num_dogs > 0:\n",
    "            output_path = Path(output_dir) / f\"detected_custom_{img_path.name}\"\n",
    "            annotated = results[0].plot()\n",
    "            cv2.imwrite(str(output_path), annotated)\n",
    "    \n",
    "    # Create results dataframe\n",
    "    results_df = pd.DataFrame(results_data)\n",
    "    return results_df\n",
    "\n",
    "# Run batch prediction on validation set\n",
    "val_images = list((PROJECT_ROOT / 'test').glob('*.*'))\n",
    "\n",
    "if val_images:\n",
    "    print(\"Running batch prediction on validation images...\\n\")\n",
    "    results_df = batch_predict(PROJECT_ROOT / 'test', OUTPUTS_DIR, model, config['confidence_threshold'])\n",
    "    \n",
    "    if results_df is not None:\n",
    "        print(\"\\nBatch Prediction Results:\")\n",
    "        print(results_df.to_string(index=False))\n",
    "        \n",
    "        print(f\"\\nSummary:\")\n",
    "        print(f\"  Total images processed: {len(results_df)}\")\n",
    "        print(f\"  Images with dogs detected: {(results_df['dogs_detected'] > 0).sum()}\")\n",
    "        print(f\"  Total dogs detected: {results_df['dogs_detected'].sum()}\")\n",
    "        print(f\"  Average confidence: {results_df['confidence_avg'].mean():.3f}\")\n",
    "        \n",
    "        # Save results to CSV\n",
    "        results_csv = OUTPUTS_DIR / 'detection_results_custom.csv'\n",
    "        results_df.to_csv(results_csv, index=False)\n",
    "        print(f\"\\nResults saved to: {results_csv}\")\n",
    "else:\n",
    "    print(\"No validation images available for batch prediction.\")\n",
    "    print(f\"Add images to: {IMAGES_DIR / 'val'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446b2890",
   "metadata": {},
   "source": [
    "## 9. Model Evaluation and Metrics\n",
    "\n",
    "Evaluate trained model performance on the test dataset with ground truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "446b2890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test images: 100\n",
      "Test labels: 100\n",
      "\n",
      "Test dataset found!\n",
      "Running model evaluation on test set...\n",
      "\n",
      "Ultralytics 8.3.232 ðŸš€ Python-3.13.7 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 3060 Ti, 7831MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n",
      "YOLO11n summary (fused): 100 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 4621.8Â±1218.0 MB/s, size: 90.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/joni/Documentos/univali/inteligencia_artificial/yolo-dog/data/labels/test.cache... 100 images, 49 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 100/100 380.6Kit/s 0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 4621.8Â±1218.0 MB/s, size: 90.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/joni/Documentos/univali/inteligencia_artificial/yolo-dog/data/labels/test.cache... 100 images, 49 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 100/100 380.6Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 9.7it/s 0.7s0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 9.7it/s 0.7s\n",
      "                   all        100         59      0.788       0.63      0.725      0.528\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
      "Results saved to \u001b[1m/home/joni/Documentos/univali/inteligencia_artificial/yolo-dog/runs/detect/val3\u001b[0m\n",
      "                   all        100         59      0.788       0.63      0.725      0.528\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
      "Results saved to \u001b[1m/home/joni/Documentos/univali/inteligencia_artificial/yolo-dog/runs/detect/val3\u001b[0m\n",
      "\n",
      "============================================================\n",
      "Evaluation Metrics\n",
      "============================================================\n",
      "  mAP@0.5:      0.7252\n",
      "  mAP@0.5:0.95: 0.5283\n",
      "  Precision:    0.7881\n",
      "  Recall:       0.6302\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Evaluation Metrics\n",
      "============================================================\n",
      "  mAP@0.5:      0.7252\n",
      "  mAP@0.5:0.95: 0.5283\n",
      "  Precision:    0.7881\n",
      "  Recall:       0.6302\n",
      "============================================================\n",
      "\n",
      "Confusion matrix saved to: /home/joni/Documentos/univali/inteligencia_artificial/yolo-dog/outputs/confusion_matrix_custom.png\n",
      "\n",
      "Confusion matrix saved to: /home/joni/Documentos/univali/inteligencia_artificial/yolo-dog/outputs/confusion_matrix_custom.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check for test dataset\n",
    "test_images = list((IMAGES_DIR / 'test').glob('*.*'))\n",
    "test_labels = list((LABELS_DIR / 'test').glob('*.txt'))\n",
    "\n",
    "print(f\"Test images: {len(test_images)}\")\n",
    "print(f\"Test labels: {len(test_labels)}\")\n",
    "\n",
    "if len(test_images) > 0 and len(test_labels) > 0:\n",
    "    print(\"\\nTest dataset found!\")\n",
    "    print(\"Running model evaluation on test set...\\n\")\n",
    "    \n",
    "    # Run validation on test set\n",
    "    metrics = model.val(\n",
    "        data=str(dataset_yaml_path),\n",
    "        split='test',\n",
    "        imgsz=config['img_size'],\n",
    "        batch=config['batch_size'],\n",
    "        conf=config['confidence_threshold'],\n",
    "        iou=config['iou_threshold'],\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Display metrics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Evaluation Metrics\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"  mAP@0.5:      {metrics.box.map50:.4f}\")\n",
    "    print(f\"  mAP@0.5:0.95: {metrics.box.map:.4f}\")\n",
    "    print(f\"  Precision:    {metrics.box.mp:.4f}\")\n",
    "    print(f\"  Recall:       {metrics.box.mr:.4f}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Plot confusion matrix if available\n",
    "    if hasattr(metrics, 'confusion_matrix') and metrics.confusion_matrix is not None:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(metrics.confusion_matrix.matrix, annot=True, fmt='g', cmap='Blues')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        cm_path = OUTPUTS_DIR / 'confusion_matrix_custom.png'\n",
    "        plt.savefig(cm_path)\n",
    "        print(f\"\\nConfusion matrix saved to: {cm_path}\")\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"\\nNo test dataset found.\")\n",
    "    print(\"To evaluate the model, add test images and labels:\")\n",
    "    print(f\"  - Images: {IMAGES_DIR / 'test'}\")\n",
    "    print(f\"  - Labels: {LABELS_DIR / 'test'}\")\n",
    "    print(\"\\nTest set evaluation requires labeled ground truth data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b34e19f",
   "metadata": {},
   "source": [
    "## 10. Model Export and Deployment\n",
    "\n",
    "Export the trained model for deployment and run inference demonstrations.\n",
    "\n",
    "**Export Formats Supported:**\n",
    "- **ONNX**: Cross-platform inference\n",
    "\n",
    "This section demonstrates model export and compares inference performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd9a7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if trained model exists\n",
    "best_model_path = MODELS_DIR / 'yolo_dog_custom' / 'weights' / 'best.pt'\n",
    "last_model_path = MODELS_DIR / 'yolo_dog_custom' / 'weights' / 'last.pt'\n",
    "\n",
    "if best_model_path.exists():\n",
    "    print(\"=\"*60)\n",
    "    print(\"Model Export and Deployment\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load the best trained model\n",
    "    print(f\"\\nLoading best trained model from: {best_model_path}\")\n",
    "    export_model = YOLO(str(best_model_path))\n",
    "    print(f\"Model loaded successfully!\")\n",
    "    \n",
    "    # Create exports directory\n",
    "    exports_dir = OUTPUTS_DIR / 'exports'\n",
    "    exports_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(f\"\\nExport directory: {exports_dir}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    print(\"Exporting model to ONNX format...\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Export to ONNX\n",
    "        onnx_path = export_model.export(\n",
    "            format='onnx',\n",
    "            imgsz=config['img_size'],\n",
    "            simplify=True,  # Simplify ONNX model\n",
    "            opset=12  # ONNX opset version\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nModel exported successfully!\")\n",
    "        print(f\"  ONNX model saved to: {onnx_path}\")\n",
    "        \n",
    "        # Get file size\n",
    "        file_size_mb = os.path.getsize(onnx_path) / (1024 * 1024)\n",
    "        print(f\"  File size: {file_size_mb:.2f} MB\")\n",
    "        \n",
    "        print(\"\\nONNX model can be used with:\")\n",
    "        print(\"  - ONNX Runtime (CPU/GPU)\")\n",
    "        print(\"  - TensorRT (NVIDIA)\")\n",
    "        print(\"  - OpenVINO (Intel)\")\n",
    "        print(\"  - Various inference frameworks\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nExport failed: {e}\")\n",
    "        print(\"\\nNote: ONNX export requires 'onnx' package:\")\n",
    "        print(\"  pip install onnx\")\n",
    "else:\n",
    "    print(\"No trained model found!\")\n",
    "    print(f\"Expected model at: {best_model_path}\")\n",
    "    print(\"\\nPlease train the model first (Section 6) before exporting.\")\n",
    "    print(\"The model will be saved after training completes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0089a0f5",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates how to train a custom YOLOv8 model on your own dog dataset for specialized detection tasks.\n",
    "\n",
    "### Key Points\n",
    "- Fine-tunes YOLOv8 on custom labeled data\n",
    "- Requires proper dataset preparation in YOLO format\n",
    "- Provides training metrics and evaluation\n",
    "- Supports inference on images and videos with trained model\n",
    "\n",
    "### Dataset Requirements\n",
    "1. **Images**: JPG/PNG format in train/val/test directories\n",
    "2. **Labels**: YOLO format .txt files (one per image)\n",
    "3. **Structure**: Organized in `data/images/` and `data/labels/` subdirectories\n",
    "4. **Format**: `class_id center_x center_y width height` (normalized 0-1)\n",
    "\n",
    "### Training Tips\n",
    "- Start with a smaller model (yolov8n.pt) for faster training\n",
    "- Use GPU for significantly faster training times\n",
    "- Monitor training metrics to avoid overfitting\n",
    "- Adjust batch_size based on available GPU memory\n",
    "- Use data augmentation (built into YOLO training)\n",
    "\n",
    "### Next Steps\n",
    "- Experiment with different model sizes (n, s, m, l, x)\n",
    "- Adjust training hyperparameters for better performance\n",
    "- Collect more diverse training data if accuracy is low\n",
    "- Export model for deployment (ONNX, TensorRT, etc.)\n",
    "\n",
    "### Useful Resources\n",
    "- [Ultralytics YOLO Documentation](https://docs.ultralytics.com/)\n",
    "- [YOLO Training Guide](https://docs.ultralytics.com/modes/train/)\n",
    "- [Dataset Preparation Tips](https://docs.ultralytics.com/datasets/)\n",
    "\n",
    "---\n",
    "\n",
    "**Project**: YOLO Dog Detection - Custom Training  \n",
    "**Date**: 2025  \n",
    "**Model**: YOLOv8 Fine-tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a58e40",
   "metadata": {},
   "source": [
    "## 11. Run Inference with Trained Model\n",
    "\n",
    "Execute inference using your trained custom model on images from a folder or live camera feed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "53f779f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Custom Model Inference\n",
      "============================================================\n",
      "\n",
      "Loaded model from: /home/joni/Documentos/univali/inteligencia_artificial/yolo-dog/models/yolo_dog_custom/weights/best.pt\n",
      "Model ready for inference!\n",
      "\n",
      "------------------------------------------------------------\n",
      "Select Inference Mode:\n",
      "------------------------------------------------------------\n",
      "\n",
      "Option 1: Process images from a folder\n",
      "Option 2: Real-time camera detection\n",
      "\n",
      "To run inference, uncomment ONE of the options below:\n",
      "------------------------------------------------------------\n",
      "\n",
      "Processing 2 images from: /home/joni/Documentos/univali/inteligencia_artificial/yolo-dog/test\n",
      "Saving results to: /home/joni/Documentos/univali/inteligencia_artificial/yolo-dog/outputs/inference_results\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 32.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing complete! Results saved to: /home/joni/Documentos/univali/inteligencia_artificial/yolo-dog/outputs/inference_results\n",
      "\n",
      "============================================================\n",
      "Instructions:\n",
      "============================================================\n",
      "1. Uncomment OPTION 1 block to process images from a folder\n",
      "2. Uncomment OPTION 2 block to use camera for real-time detection\n",
      "3. Run this cell after uncommenting your chosen option\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the best trained model\n",
    "best_model_path = MODELS_DIR / 'yolo_dog_custom' / 'weights' / 'best.pt'\n",
    "\n",
    "if not best_model_path.exists():\n",
    "    print(\"No trained model found!\")\n",
    "    print(f\"Expected model at: {best_model_path}\")\n",
    "    print(\"\\nPlease train the model first (Section 6) before running inference.\")\n",
    "else:\n",
    "    print(\"=\"*60)\n",
    "    print(\"Custom Model Inference\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nLoaded model from: {best_model_path}\")\n",
    "    inference_model = YOLO(str(best_model_path))\n",
    "    print(f\"Model ready for inference!\")\n",
    "    \n",
    "    # Choose inference mode\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"Select Inference Mode:\")\n",
    "    print(\"-\"*60)\n",
    "    print(\"\\nOption 1: Process images from a folder\")\n",
    "    print(\"Option 2: Real-time camera detection\")\n",
    "    print(\"\\nTo run inference, uncomment ONE of the options below:\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    # ============================================================\n",
    "    # OPTION 1: Process Images from Folder\n",
    "    # ============================================================\n",
    "    # Uncomment the block below to process images from a folder\n",
    "    \n",
    "    inference_folder = PROJECT_ROOT / 'test'  # Change to your folder path\n",
    "    output_folder = OUTPUTS_DIR / 'inference_results'\n",
    "    output_folder.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Get all images in folder\n",
    "    image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp']\n",
    "    images_to_process = []\n",
    "    for ext in image_extensions:\n",
    "        images_to_process.extend(inference_folder.glob(ext))\n",
    "    \n",
    "    if len(images_to_process) == 0:\n",
    "        print(f\"\\nNo images found in {inference_folder}\")\n",
    "        print(\"Please add images to the folder and try again.\")\n",
    "    else:\n",
    "        print(f\"\\nProcessing {len(images_to_process)} images from: {inference_folder}\")\n",
    "        print(f\"Saving results to: {output_folder}\")\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        for img_path in tqdm(images_to_process, desc=\"Processing images\"):\n",
    "            # Run inference\n",
    "            results = inference_model(\n",
    "                str(img_path),\n",
    "                conf=config['confidence_threshold'],\n",
    "                iou=config['iou_threshold'],\n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            # Save annotated image\n",
    "            num_detections = len(results[0].boxes)\n",
    "            if num_detections > 0:\n",
    "                output_path = output_folder / f\"detected_{img_path.name}\"\n",
    "                annotated = results[0].plot()\n",
    "                cv2.imwrite(str(output_path), annotated)\n",
    "                print(f\"  {img_path.name}: {num_detections} dog(s) detected\")\n",
    "            else:\n",
    "                # Save even if no detections\n",
    "                output_path = output_folder / f\"no_detection_{img_path.name}\"\n",
    "                annotated = results[0].plot()\n",
    "                cv2.imwrite(str(output_path), annotated)\n",
    "        \n",
    "        print(f\"\\nProcessing complete! Results saved to: {output_folder}\")\n",
    "    \n",
    "    # ============================================================\n",
    "    # OPTION 2: Real-time Camera Detection\n",
    "    # ============================================================\n",
    "    # Uncomment the block below to use camera for real-time detection\n",
    "    \n",
    "    # camera_index = 0  # Usually 0 for default camera, try 1, 2, etc. if not working\n",
    "    \n",
    "    # print(f\"\\nStarting camera detection (Camera index: {camera_index})...\")\n",
    "    # print(\"Press 'q' to quit, 's' to save current frame\")\n",
    "    # print(\"-\"*60)\n",
    "    \n",
    "    # cap = cv2.VideoCapture(camera_index)\n",
    "    \n",
    "    # if not cap.isOpened():\n",
    "    #     print(f\"\\nError: Could not open camera {camera_index}\")\n",
    "    #     print(\"Try changing camera_index to 1, 2, etc.\")\n",
    "    # else:\n",
    "    #     # Set camera properties\n",
    "    #     cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "    #     cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "        \n",
    "    #     frame_count = 0\n",
    "    #     fps_time = time.time()\n",
    "    #     fps = 0\n",
    "        \n",
    "    #     camera_output_dir = OUTPUTS_DIR / 'camera_captures'\n",
    "    #     camera_output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "    #     print(\"Camera opened successfully! Window should appear...\")\n",
    "        \n",
    "    #     try:\n",
    "    #         while True:\n",
    "    #             ret, frame = cap.read()\n",
    "                \n",
    "    #             if not ret:\n",
    "    #                 print(\"Failed to grab frame\")\n",
    "    #                 break\n",
    "                \n",
    "    #             # Run inference\n",
    "    #             results = inference_model(\n",
    "    #                 frame,\n",
    "    #                 conf=config['confidence_threshold'],\n",
    "    #                 iou=config['iou_threshold'],\n",
    "    #                 verbose=False\n",
    "    #             )\n",
    "                \n",
    "    #             # Get annotated frame\n",
    "    #             annotated_frame = results[0].plot()\n",
    "                \n",
    "    #             # Calculate FPS\n",
    "    #             frame_count += 1\n",
    "    #             if frame_count % 30 == 0:\n",
    "    #                 fps = 30 / (time.time() - fps_time)\n",
    "    #                 fps_time = time.time()\n",
    "                \n",
    "    #             # Add FPS and detection count to frame\n",
    "    #             num_detections = len(results[0].boxes)\n",
    "    #             cv2.putText(annotated_frame, f\"FPS: {fps:.1f}\", (10, 30), \n",
    "    #                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    #             cv2.putText(annotated_frame, f\"Dogs: {num_detections}\", (10, 70), \n",
    "    #                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                \n",
    "    #             # Display frame\n",
    "    #             cv2.imshow('YOLO Dog Detection - Press Q to quit, S to save', annotated_frame)\n",
    "                \n",
    "    #             # Handle key presses\n",
    "    #             key = cv2.waitKey(1) & 0xFF\n",
    "                \n",
    "    #             if key == ord('q'):\n",
    "    #                 print(\"\\nQuitting camera detection...\")\n",
    "    #                 break\n",
    "    #             elif key == ord('s'):\n",
    "    #                 # Save current frame\n",
    "    #                 timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    #                 save_path = camera_output_dir / f\"capture_{timestamp}.jpg\"\n",
    "    #                 cv2.imwrite(str(save_path), annotated_frame)\n",
    "    #                 print(f\"Frame saved: {save_path}\")\n",
    "        \n",
    "    #     except KeyboardInterrupt:\n",
    "    #         print(\"\\nInterrupted by user\")\n",
    "        \n",
    "    #     finally:\n",
    "    #         cap.release()\n",
    "    #         cv2.destroyAllWindows()\n",
    "    #         print(f\"\\nCamera released. Total frames processed: {frame_count}\")\n",
    "    #         if camera_output_dir.exists():\n",
    "    #             saved_frames = list(camera_output_dir.glob('*.jpg'))\n",
    "    #             if saved_frames:\n",
    "    #                 print(f\"Saved frames: {len(saved_frames)} in {camera_output_dir}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Instructions:\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"1. Uncomment OPTION 1 block to process images from a folder\")\n",
    "    print(\"2. Uncomment OPTION 2 block to use camera for real-time detection\")\n",
    "    print(\"3. Run this cell after uncommenting your chosen option\")\n",
    "    print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
