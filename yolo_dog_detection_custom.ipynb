{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46a22f67",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import all necessary libraries for YOLO training, computer vision, and data visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d07551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Libraries\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# YOLO and Deep Learning\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import urllib\n",
    "from pycocotools.coco import COCO\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Computer Vision\n",
    "import cv2\n",
    "\n",
    "# Data Science and Visualization\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Utilities\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check if GPU is available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "if device == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"Training on CPU will be significantly slower than GPU\")\n",
    "    print(\"Consider using Google Colab or a GPU-enabled environment\")\n",
    "\n",
    "# Set matplotlib style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b988d6",
   "metadata": {},
   "source": [
    "## 2. Setup Project Directories and Configuration\n",
    "\n",
    "Create necessary directories for custom dataset training and define training configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508dd494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project Configuration\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "IMAGES_DIR = DATA_DIR / 'images'\n",
    "LABELS_DIR = DATA_DIR / 'labels'\n",
    "MODELS_DIR = PROJECT_ROOT / 'models'\n",
    "OUTPUTS_DIR = PROJECT_ROOT / 'outputs'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for directory in [DATA_DIR, IMAGES_DIR, LABELS_DIR, MODELS_DIR, OUTPUTS_DIR]:\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create train/val/test subdirectories\n",
    "for split in ['train', 'val', 'test']:\n",
    "    (IMAGES_DIR / split).mkdir(exist_ok=True)\n",
    "    (LABELS_DIR / split).mkdir(exist_ok=True)\n",
    "\n",
    "# Configuration dictionary - Training settings\n",
    "config = {\n",
    "    'model_name': 'yolo11n.pt',  # Base model to fine-tune\n",
    "    'img_size': 640,\n",
    "    'batch_size': 16,\n",
    "    'epochs': 50,\n",
    "    'confidence_threshold': 0.001,\n",
    "    'iou_threshold': 0.45,\n",
    "    'dog_class_id': 0,  # Custom dataset - single class (dog)\n",
    "}\n",
    "\n",
    "print(\"Project Configuration:\")\n",
    "print(f\"├── Data Directory: {DATA_DIR}\")\n",
    "print(f\"├── Images Directory: {IMAGES_DIR}\")\n",
    "print(f\"│   ├── train: {IMAGES_DIR / 'train'}\")\n",
    "print(f\"│   ├── val: {IMAGES_DIR / 'val'}\")\n",
    "print(f\"│   └── test: {IMAGES_DIR / 'test'}\")\n",
    "print(f\"├── Labels Directory: {LABELS_DIR}\")\n",
    "print(f\"│   ├── train: {LABELS_DIR / 'train'}\")\n",
    "print(f\"│   ├── val: {LABELS_DIR / 'val'}\")\n",
    "print(f\"│   └── test: {LABELS_DIR / 'test'}\")\n",
    "print(f\"├── Models Directory: {MODELS_DIR}\")\n",
    "print(f\"└── Outputs Directory: {OUTPUTS_DIR}\")\n",
    "print(f\"\\nTraining Configuration:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0915164",
   "metadata": {},
   "source": [
    "## 2.5. Download and Format COCO Dataset for Training (Optional)\n",
    "\n",
    "Download COCO 2017 dataset, filter for dog images, and convert to YOLO format for training.\n",
    "\n",
    "**What this section does:**\n",
    "- Downloads COCO 2017 test dataset annotations\n",
    "- Filters images containing dogs (COCO class_id: 18)\n",
    "- Downloads filtered images\n",
    "- Converts COCO bbox format to YOLO format\n",
    "- Splits data into train (70%), val (20%), test (10%)\n",
    "- Organizes into proper directory structure\n",
    "\n",
    "**Note:** Skip this section if you already have a custom dataset prepared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb59773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for COCO download\n",
    "COCO_CONFIG = {\n",
    "    'annotations_url': 'http://images.cocodataset.org/annotations/annotations_trainval2017.zip',\n",
    "    'images_base_url': 'http://images.cocodataset.org/train2017/',\n",
    "    'dog_class_id': 18,  # Dog class in COCO dataset\n",
    "    'max_samples': None,  # Maximum images to download (set None for all dog images)\n",
    "    'train_split': 0.7, # Data to train the model\n",
    "    'val_split': 0.2, # Data to validate the model during training\n",
    "    'test_split': 0.1 # Data to test the model after training\n",
    "}\n",
    "\n",
    "def download_file(url, dest_path):\n",
    "    \"\"\"Download file with progress bar\"\"\"\n",
    "    print(f\"Downloading {url}...\")\n",
    "    try:\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            total_size = int(response.headers.get('content-length', 0))\n",
    "            with open(dest_path, 'wb') as f, tqdm(total=total_size, unit='B', unit_scale=True) as pbar:\n",
    "                while True:\n",
    "                    chunk = response.read(8192)\n",
    "                    if not chunk:\n",
    "                        break\n",
    "                    f.write(chunk)\n",
    "                    pbar.update(len(chunk))\n",
    "        print(f\"Downloaded to {dest_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading: {e}\")\n",
    "        return False\n",
    "\n",
    "def coco_to_yolo_bbox(bbox, img_width, img_height):\n",
    "    \"\"\"\n",
    "    Convert COCO bbox format to YOLO format\n",
    "    COCO: [x_min, y_min, width, height] (absolute pixels)\n",
    "    YOLO: [center_x, center_y, width, height] (normalized 0-1)\n",
    "    \"\"\"\n",
    "    x_min, y_min, width, height = bbox\n",
    "    \n",
    "    # Calculate center coordinates\n",
    "    center_x = (x_min + width / 2) / img_width\n",
    "    center_y = (y_min + height / 2) / img_height\n",
    "    \n",
    "    # Normalize width and height\n",
    "    norm_width = width / img_width\n",
    "    norm_height = height / img_height\n",
    "    \n",
    "    return [center_x, center_y, norm_width, norm_height]\n",
    "\n",
    "def download_and_format_coco_dogs():\n",
    "    \"\"\"Main function to download and format COCO dog dataset\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"COCO Dog Dataset Download and Formatting\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create temporary directory for annotations\n",
    "    temp_dir = PROJECT_ROOT / 'temp_coco'\n",
    "    temp_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Download annotations\n",
    "    annotations_zip = temp_dir / 'annotations_trainval2017.zip'\n",
    "    if not annotations_zip.exists():\n",
    "        if not download_file(COCO_CONFIG['annotations_url'], annotations_zip):\n",
    "            print(\"Failed to download annotations\")\n",
    "            return\n",
    "        \n",
    "        # Extract annotations\n",
    "        print(\"Extracting annotations...\")\n",
    "        shutil.unpack_archive(annotations_zip, temp_dir)\n",
    "    \n",
    "    # Load COCO annotations\n",
    "    annotations_file = temp_dir / 'annotations' / 'instances_train2017.json'\n",
    "    print(f\"\\nLoading COCO annotations from {annotations_file}\")\n",
    "    coco = COCO(str(annotations_file))\n",
    "    \n",
    "    # Get all images with dogs\n",
    "    dog_class_id = COCO_CONFIG['dog_class_id']\n",
    "    dog_img_ids = coco.getImgIds(catIds=[dog_class_id])\n",
    "    print(f\"Found {len(dog_img_ids)} images containing dogs in COCO val2017\")\n",
    "    \n",
    "    # Limit samples if specified\n",
    "    if COCO_CONFIG['max_samples'] and len(dog_img_ids) > COCO_CONFIG['max_samples']:\n",
    "        dog_img_ids = dog_img_ids[:COCO_CONFIG['max_samples']]\n",
    "        print(f\"Limiting to {COCO_CONFIG['max_samples']} images\")\n",
    "    \n",
    "    # Get random non-dog images for test set (negative examples)\n",
    "    all_img_ids = coco.getImgIds()\n",
    "    non_dog_img_ids = [img_id for img_id in all_img_ids if img_id not in dog_img_ids]\n",
    "    \n",
    "    # Sample random non-dog images (about 50% of test set will be non-dog images)\n",
    "    num_test_dogs = int(len(dog_img_ids) * COCO_CONFIG['test_split'])\n",
    "    num_non_dog_test = num_test_dogs  # Equal number = 50% each\n",
    "    random.seed(42)\n",
    "    non_dog_test_ids = random.sample(non_dog_img_ids, min(num_non_dog_test, len(non_dog_img_ids)))\n",
    "    print(f\"Selected {len(non_dog_test_ids)} random non-dog images for test set (50% of test data)\")\n",
    "    \n",
    "    # Split into train/val/test\n",
    "    train_ratio = COCO_CONFIG['train_split']\n",
    "    val_ratio = COCO_CONFIG['val_split']\n",
    "    test_ratio = COCO_CONFIG['test_split']\n",
    "    \n",
    "    # First split: separate test set (only from dog images)\n",
    "    train_val_ids, test_dog_ids = train_test_split(\n",
    "        dog_img_ids, \n",
    "        test_size=test_ratio, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Add non-dog images to test set\n",
    "    test_ids = test_dog_ids + non_dog_test_ids\n",
    "    \n",
    "    # Second split: separate train and val\n",
    "    val_size = val_ratio / (train_ratio + val_ratio)\n",
    "    train_ids, val_ids = train_test_split(\n",
    "        train_val_ids, \n",
    "        test_size=val_size, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    splits = {\n",
    "        'train': train_ids,\n",
    "        'val': val_ids,\n",
    "        'test': test_ids\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nDataset splits:\")\n",
    "    print(f\"  Train: {len(train_ids)} images ({train_ratio*100:.0f}%) - all with dogs\")\n",
    "    print(f\"  Val:   {len(val_ids)} images ({val_ratio*100:.0f}%) - all with dogs\")\n",
    "    print(f\"  Test:  {len(test_ids)} images ({len(test_dog_ids)} dogs + {len(non_dog_test_ids)} non-dogs)\")\n",
    "    \n",
    "    # Process each split\n",
    "    for split_name, img_ids in splits.items():\n",
    "        print(f\"\\nProcessing {split_name} set...\")\n",
    "        images_split_dir = IMAGES_DIR / split_name\n",
    "        labels_split_dir = LABELS_DIR / split_name\n",
    "        \n",
    "        # Clear existing files in split\n",
    "        for f in images_split_dir.glob('*'):\n",
    "            f.unlink()\n",
    "        for f in labels_split_dir.glob('*.txt'):\n",
    "            f.unlink()\n",
    "        \n",
    "        for img_id in tqdm(img_ids, desc=f\"Downloading {split_name}\"):\n",
    "            # Get image info\n",
    "            img_info = coco.loadImgs(img_id)[0]\n",
    "            img_filename = img_info['file_name']\n",
    "            img_width = img_info['width']\n",
    "            img_height = img_info['height']\n",
    "            \n",
    "            # Download image\n",
    "            img_url = COCO_CONFIG['images_base_url'] + img_filename\n",
    "            img_path = images_split_dir / img_filename\n",
    "            \n",
    "            try:\n",
    "                urllib.request.urlretrieve(img_url, img_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to download {img_filename}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Get annotations for this image (only dogs)\n",
    "            ann_ids = coco.getAnnIds(imgIds=img_id, catIds=[dog_class_id])\n",
    "            annotations = coco.loadAnns(ann_ids)\n",
    "            \n",
    "            # Convert to YOLO format and save\n",
    "            label_path = labels_split_dir / img_filename.replace('.jpg', '.txt')\n",
    "            \n",
    "            # For non-dog images (negative examples), create empty label file\n",
    "            if len(annotations) == 0:\n",
    "                # Create empty label file for images with no dogs\n",
    "                label_path.touch()\n",
    "            else:\n",
    "                # Write dog annotations\n",
    "                with open(label_path, 'w') as f:\n",
    "                    for ann in annotations:\n",
    "                        bbox = ann['bbox']\n",
    "                        yolo_bbox = coco_to_yolo_bbox(bbox, img_width, img_height)\n",
    "                        # YOLO format: class_id center_x center_y width height\n",
    "                        f.write(f\"0 {yolo_bbox[0]:.6f} {yolo_bbox[1]:.6f} {yolo_bbox[2]:.6f} {yolo_bbox[3]:.6f}\\n\")\n",
    "    \n",
    "    # Clean up temporary files\n",
    "    print(\"\\nCleaning up temporary files...\")\n",
    "    shutil.rmtree(temp_dir)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COCO Dataset Download Complete!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Images saved to: {IMAGES_DIR}\")\n",
    "    print(f\"Labels saved to: {LABELS_DIR}\")\n",
    "    print(\"\\nDataset is ready for training!\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Option to download COCO dataset\n",
    "print(\"COCO Dog Dataset Preparation\")\n",
    "print(\"-\" * 60)\n",
    "print(\"This will download COCO 2017 validation images containing dogs\")\n",
    "print(f\"and format them for YOLO training.\\n\")\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  - Max samples: {COCO_CONFIG['max_samples']}\")\n",
    "print(f\"  - Train/Val/Test split: {COCO_CONFIG['train_split']}/{COCO_CONFIG['val_split']}/{COCO_CONFIG['test_split']}\")\n",
    "print(f\"  - Target directory: {DATA_DIR}\")\n",
    "print(\"\\nTo download and format COCO dataset, run:\")\n",
    "print(\"  download_and_format_coco_dogs()\")\n",
    "print(\"\\nOr skip this cell if you have your own dataset ready.\")\n",
    "\n",
    "# Uncomment the line below to automatically download COCO dataset\n",
    "download_and_format_coco_dogs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc5c9fe",
   "metadata": {},
   "source": [
    "## 3. Define Helper Functions\n",
    "\n",
    "Create utility functions for image processing, visualization, training metrics, and dog detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0508ffba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_detections(image_path, results, save_path=None):\n",
    "    \"\"\"Visualize detection results with bounding boxes\"\"\"\n",
    "    # Read image\n",
    "    img = cv2.imread(str(image_path))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Plot results\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "    ax.imshow(img)\n",
    "    \n",
    "    # Draw bounding boxes\n",
    "    if results and len(results[0].boxes) > 0:\n",
    "        for box in results[0].boxes:\n",
    "            # Get box coordinates\n",
    "            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "            confidence = box.conf[0].cpu().numpy()\n",
    "            class_id = int(box.cls[0].cpu().numpy())\n",
    "            \n",
    "            # Draw rectangle\n",
    "            rect = plt.Rectangle((x1, y1), x2-x1, y2-y1, \n",
    "                                fill=False, edgecolor='red', linewidth=2)\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "            # Add label\n",
    "            label = f\"Dog {confidence:.2f}\"\n",
    "            ax.text(x1, y1-10, label, color='red', fontsize=12,\n",
    "                   bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight', dpi=150)\n",
    "        print(f\"Saved visualization to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "def process_video_detections(video_path, model, output_path, conf_threshold=0.25):\n",
    "    \"\"\"Process video file and detect dogs frame by frame\"\"\"\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    \n",
    "    # Get video properties\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # Define codec and create VideoWriter\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(str(output_path), fourcc, fps, (width, height))\n",
    "    \n",
    "    print(f\"Processing video: {total_frames} frames at {fps} FPS\")\n",
    "    \n",
    "    frame_count = 0\n",
    "    with tqdm(total=total_frames) as pbar:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # Run detection\n",
    "            results = model(frame, conf=conf_threshold, verbose=False)\n",
    "            \n",
    "            # Draw results on frame\n",
    "            annotated_frame = results[0].plot()\n",
    "            \n",
    "            # Write frame\n",
    "            out.write(annotated_frame)\n",
    "            \n",
    "            frame_count += 1\n",
    "            pbar.update(1)\n",
    "    \n",
    "    cap.release()\n",
    "    out.release()\n",
    "    \n",
    "    print(f\"Processed {frame_count} frames. Output saved to {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "def display_metrics(results_df):\n",
    "    \"\"\"Display training metrics and create visualizations\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Plot losses\n",
    "    if 'train/box_loss' in results_df.columns:\n",
    "        axes[0, 0].plot(results_df['epoch'], results_df['train/box_loss'], label='Box Loss')\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].set_title('Training Box Loss')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True)\n",
    "    \n",
    "    # Plot mAP\n",
    "    if 'metrics/mAP50(B)' in results_df.columns:\n",
    "        axes[0, 1].plot(results_df['epoch'], results_df['metrics/mAP50(B)'], label='mAP@0.5')\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('mAP')\n",
    "        axes[0, 1].set_title('Mean Average Precision')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True)\n",
    "    \n",
    "    # Plot precision and recall\n",
    "    if 'metrics/precision(B)' in results_df.columns:\n",
    "        axes[1, 0].plot(results_df['epoch'], results_df['metrics/precision(B)'], label='Precision')\n",
    "        axes[1, 0].plot(results_df['epoch'], results_df['metrics/recall(B)'], label='Recall')\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('Score')\n",
    "        axes[1, 0].set_title('Precision and Recall')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True)\n",
    "    \n",
    "    # Summary statistics\n",
    "    axes[1, 1].axis('off')\n",
    "    if not results_df.empty:\n",
    "        summary_text = f\"Training Summary\\n\\n\"\n",
    "        summary_text += f\"Total Epochs: {len(results_df)}\\n\"\n",
    "        if 'metrics/mAP50(B)' in results_df.columns:\n",
    "            summary_text += f\"Best mAP@0.5: {results_df['metrics/mAP50(B)'].max():.4f}\\n\"\n",
    "        if 'metrics/precision(B)' in results_df.columns:\n",
    "            summary_text += f\"Best Precision: {results_df['metrics/precision(B)'].max():.4f}\\n\"\n",
    "        if 'metrics/recall(B)' in results_df.columns:\n",
    "            summary_text += f\"Best Recall: {results_df['metrics/recall(B)'].max():.4f}\\n\"\n",
    "        \n",
    "        axes[1, 1].text(0.1, 0.5, summary_text, fontsize=14, family='monospace',\n",
    "                       verticalalignment='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "print(\"Helper functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae767e6",
   "metadata": {},
   "source": [
    "## 4. Load Base YOLO Model\n",
    "\n",
    "Initialize YOLO model with pre-trained COCO weights as a starting point for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7ffe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained YOLO model (will be fine-tuned)\n",
    "print(f\"Loading {config['model_name']} model as base...\")\n",
    "model = YOLO(config['model_name'])\n",
    "\n",
    "# Display model information\n",
    "print(f\"\\nBase model loaded successfully!\")\n",
    "print(f\"Model type: {type(model)}\")\n",
    "print(f\"Device: {model.device}\")\n",
    "print(\"\\nThis model will be fine-tuned on your custom dog dataset.\")\n",
    "print(\"The pre-trained weights provide a good starting point for training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c394c46",
   "metadata": {},
   "source": [
    "## 5. Prepare Custom Dataset for Training\n",
    "\n",
    "Set up the dataset configuration file and verify dataset structure.\n",
    "\n",
    "**Required**: Your dataset must follow YOLO format:\n",
    "- Images in `data/images/train/`, `data/images/val/`, `data/images/test/`\n",
    "- Labels in `data/labels/train/`, `data/labels/val/`, `data/labels/test/`\n",
    "- Each image has a corresponding `.txt` label file\n",
    "- Label format: `class_id center_x center_y width height` (all normalized 0-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c394c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset configuration file for training\n",
    "dataset_config = {\n",
    "    'path': str(DATA_DIR.absolute()),\n",
    "    'train': 'images/train',\n",
    "    'val': 'images/val',\n",
    "    'test': 'images/test',\n",
    "    'names': {\n",
    "        0: 'dog'\n",
    "    },\n",
    "    'nc': 1  # number of classes\n",
    "}\n",
    "\n",
    "# Save dataset configuration\n",
    "dataset_yaml_path = DATA_DIR / 'dataset.yaml'\n",
    "with open(dataset_yaml_path, 'w') as f:\n",
    "    yaml.dump(dataset_config, f, default_flow_style=False)\n",
    "\n",
    "print(f\"Dataset configuration saved to: {dataset_yaml_path}\")\n",
    "print(\"\\nDataset Configuration:\")\n",
    "print(yaml.dump(dataset_config, default_flow_style=False))\n",
    "\n",
    "# Verify dataset structure\n",
    "print(\"\\nVerifying dataset structure...\")\n",
    "train_images = list((IMAGES_DIR / 'train').glob('*.*'))\n",
    "train_labels = list((LABELS_DIR / 'train').glob('*.txt'))\n",
    "val_images = list((IMAGES_DIR / 'val').glob('*.*'))\n",
    "val_labels = list((LABELS_DIR / 'val').glob('*.txt'))\n",
    "test_images = list((IMAGES_DIR / 'test').glob('*.*'))\n",
    "test_labels = list((LABELS_DIR / 'test').glob('*.txt'))\n",
    "\n",
    "print(f\"\\nDataset Summary:\")\n",
    "print(f\"├── Training set:\")\n",
    "print(f\"│   ├── Images: {len(train_images)}\")\n",
    "print(f\"│   └── Labels: {len(train_labels)}\")\n",
    "print(f\"├── Validation set:\")\n",
    "print(f\"│   ├── Images: {len(val_images)}\")\n",
    "print(f\"│   └── Labels: {len(val_labels)}\")\n",
    "print(f\"└── Test set:\")\n",
    "print(f\"    ├── Images: {len(test_images)}\")\n",
    "print(f\"    └── Labels: {len(test_labels)}\")\n",
    "\n",
    "# Check if dataset is ready\n",
    "if len(train_images) > 0 and len(val_images) > 0:\n",
    "    print(f\"\\nDataset is ready for training!\")\n",
    "    if len(train_labels) != len(train_images):\n",
    "        print(f\"Warning: Number of training labels ({len(train_labels)}) doesn't match images ({len(train_images)})\")\n",
    "    if len(val_labels) != len(val_images):\n",
    "        print(f\"Warning: Number of validation labels ({len(val_labels)}) doesn't match images ({len(val_images)})\")\n",
    "else:\n",
    "    print(f\"\\nDataset not ready!\")\n",
    "    print(f\"   Please add labeled images to:\")\n",
    "    print(f\"   - Training: {IMAGES_DIR / 'train'} and {LABELS_DIR / 'train'}\")\n",
    "    print(f\"   - Validation: {IMAGES_DIR / 'val'} and {LABELS_DIR / 'val'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cf1f5c",
   "metadata": {},
   "source": [
    "## 6. Train Custom Model\n",
    "\n",
    "Fine-tune YOLOv8 on your custom dog dataset. This section requires properly prepared training data.\n",
    "\n",
    "**Training Parameters:**\n",
    "- `epochs`: Number of training iterations (default: 50)\n",
    "- `batch_size`: Images per batch (default: 16, adjust based on GPU memory)\n",
    "- `img_size`: Input image size (default: 640)\n",
    "- `patience`: Early stopping patience (stops if no improvement for N epochs)\n",
    "\n",
    "**Note:** Training time varies based on:\n",
    "- Dataset size\n",
    "- Hardware (GPU vs CPU)\n",
    "- Number of epochs\n",
    "- Image resolution\n",
    "\n",
    "Expect ~1-2 hours on GPU for a small dataset, much longer on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cf1f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if training data exists\n",
    "train_images = list((IMAGES_DIR / 'train').glob('*.*'))\n",
    "val_images = list((IMAGES_DIR / 'val').glob('*.*'))\n",
    "\n",
    "print(f\"Training images found: {len(train_images)}\")\n",
    "print(f\"Validation images found: {len(val_images)}\")\n",
    "\n",
    "if len(train_images) > 0 and len(val_images) > 0:\n",
    "    print(\"\\nDataset ready for training!\")\n",
    "    print(f\"\\nStarting training with:\")\n",
    "    print(f\"  - Device: {device}\")\n",
    "    print(f\"  - Epochs: {config['epochs']}\")\n",
    "    print(f\"  - Batch size: {config['batch_size']}\")\n",
    "    print(f\"  - Image size: {config['img_size']}\")\n",
    "    print(f\"\\nThis may take a while...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Start training\n",
    "    results = model.train(\n",
    "        data=str(dataset_yaml_path),\n",
    "        epochs=config['epochs'],\n",
    "        imgsz=config['img_size'],\n",
    "        batch=config['batch_size'],\n",
    "        name='yolo_dog_custom',\n",
    "        project=str(MODELS_DIR),\n",
    "        patience=10,\n",
    "        save=True,\n",
    "        plots=True,\n",
    "        device=device,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Training completed!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nModel saved to: {MODELS_DIR / 'yolo_dog_custom'}\")\n",
    "    print(f\"Best weights: {MODELS_DIR / 'yolo_dog_custom' / 'weights' / 'best.pt'}\")\n",
    "    print(f\"Last weights: {MODELS_DIR / 'yolo_dog_custom' / 'weights' / 'last.pt'}\")\n",
    "    \n",
    "    # Load best model for inference\n",
    "    best_model_path = MODELS_DIR / 'yolo_dog_custom' / 'weights' / 'best.pt'\n",
    "    if best_model_path.exists():\n",
    "        model = YOLO(str(best_model_path))\n",
    "        print(f\"\\nLoaded best model for inference\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nNo training data found!\")\n",
    "    print(\"Please add labeled images to train the model:\")\n",
    "    print(f\"  - Add images to: {IMAGES_DIR / 'train'}\")\n",
    "    print(f\"  - Add labels to: {LABELS_DIR / 'train'}\")\n",
    "    print(f\"  - Add images to: {IMAGES_DIR / 'val'}\")\n",
    "    print(f\"  - Add labels to: {LABELS_DIR / 'val'}\")\n",
    "    print(\"\\nLabel Format (YOLO):\")\n",
    "    print(\"   Each .txt file should contain: class_id center_x center_y width height\")\n",
    "    print(\"   Example: 0 0.5 0.5 0.3 0.4\")\n",
    "    print(\"   All values are normalized (0-1) relative to image dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a23f7a6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Model execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1349e928",
   "metadata": {},
   "source": [
    "## 7. Video Processing with Trained Model\n",
    "\n",
    "Process video files using your trained custom model to detect dogs in real-time video streams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad274cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for video files\n",
    "video_extensions = ['*.mp4', '*.avi', '*.mov', '*.mkv']\n",
    "video_files = []\n",
    "for ext in video_extensions:\n",
    "    video_files.extend(IMAGES_DIR.glob(ext))\n",
    "\n",
    "print(f\"Video files found: {len(video_files)}\")\n",
    "for video in video_files:\n",
    "    print(f\"  - {video.name}\")\n",
    "\n",
    "if video_files:\n",
    "    for video_path in video_files:\n",
    "        print(f\"\\nProcessing video: {video_path.name}\")\n",
    "        output_path = OUTPUTS_DIR / f\"detected_custom_{video_path.name}\"\n",
    "        \n",
    "        process_video_detections(\n",
    "            video_path=video_path,\n",
    "            model=model,\n",
    "            output_path=output_path,\n",
    "            conf_threshold=config['confidence_threshold']\n",
    "        )\n",
    "        \n",
    "        print(f\"Output video saved to: {output_path}\")\n",
    "else:\n",
    "    print(\"\\nNo video files found.\")\n",
    "    print(f\"Add video files to: {IMAGES_DIR}\")\n",
    "    print(\"\\nSupported formats: MP4, AVI, MOV, MKV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bdac2d",
   "metadata": {},
   "source": [
    "## 8. Batch Prediction on Custom Images\n",
    "\n",
    "Run inference on multiple images and save results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ffdf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch prediction on all images\n",
    "def batch_predict(image_dir, output_dir, model, conf_threshold=0.25):\n",
    "    \"\"\"Run prediction on all images in a directory\"\"\"\n",
    "    image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp']\n",
    "    all_images = []\n",
    "    for ext in image_extensions:\n",
    "        all_images.extend(Path(image_dir).glob(ext))\n",
    "    \n",
    "    if not all_images:\n",
    "        print(f\"No images found in {image_dir}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Processing {len(all_images)} images...\")\n",
    "    results_data = []\n",
    "    \n",
    "    for img_path in tqdm(all_images):\n",
    "        # Run inference\n",
    "        results = model(\n",
    "            str(img_path),\n",
    "            conf=conf_threshold,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Count detections\n",
    "        num_dogs = len(results[0].boxes)\n",
    "        \n",
    "        # Save results\n",
    "        results_data.append({\n",
    "            'image': img_path.name,\n",
    "            'dogs_detected': num_dogs,\n",
    "            'confidence_avg': results[0].boxes.conf.mean().item() if num_dogs > 0 else 0\n",
    "        })\n",
    "        \n",
    "        # Save annotated image\n",
    "        if num_dogs > 0:\n",
    "            output_path = Path(output_dir) / f\"detected_custom_{img_path.name}\"\n",
    "            annotated = results[0].plot()\n",
    "            cv2.imwrite(str(output_path), annotated)\n",
    "    \n",
    "    # Create results dataframe\n",
    "    results_df = pd.DataFrame(results_data)\n",
    "    return results_df\n",
    "\n",
    "# Run batch prediction on validation set\n",
    "val_images = list((IMAGES_DIR / 'val').glob('*.*'))\n",
    "\n",
    "if val_images:\n",
    "    print(\"Running batch prediction on validation images...\\n\")\n",
    "    results_df = batch_predict(IMAGES_DIR / 'val', OUTPUTS_DIR, model, config['confidence_threshold'])\n",
    "    \n",
    "    if results_df is not None:\n",
    "        print(\"\\nBatch Prediction Results:\")\n",
    "        print(results_df.to_string(index=False))\n",
    "        \n",
    "        print(f\"\\nSummary:\")\n",
    "        print(f\"  Total images processed: {len(results_df)}\")\n",
    "        print(f\"  Images with dogs detected: {(results_df['dogs_detected'] > 0).sum()}\")\n",
    "        print(f\"  Total dogs detected: {results_df['dogs_detected'].sum()}\")\n",
    "        print(f\"  Average confidence: {results_df['confidence_avg'].mean():.3f}\")\n",
    "        \n",
    "        # Save results to CSV\n",
    "        results_csv = OUTPUTS_DIR / 'detection_results_custom.csv'\n",
    "        results_df.to_csv(results_csv, index=False)\n",
    "        print(f\"\\nResults saved to: {results_csv}\")\n",
    "else:\n",
    "    print(\"No validation images available for batch prediction.\")\n",
    "    print(f\"Add images to: {IMAGES_DIR / 'val'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446b2890",
   "metadata": {},
   "source": [
    "## 9. Model Evaluation and Metrics\n",
    "\n",
    "Evaluate trained model performance on the test dataset with ground truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446b2890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for test dataset\n",
    "test_images = list((IMAGES_DIR / 'test').glob('*.*'))\n",
    "test_labels = list((LABELS_DIR / 'test').glob('*.txt'))\n",
    "\n",
    "print(f\"Test images: {len(test_images)}\")\n",
    "print(f\"Test labels: {len(test_labels)}\")\n",
    "\n",
    "if len(test_images) > 0 and len(test_labels) > 0:\n",
    "    print(\"\\nTest dataset found!\")\n",
    "    print(\"Running model evaluation on test set...\\n\")\n",
    "    \n",
    "    # Run validation on test set\n",
    "    metrics = model.val(\n",
    "        data=str(dataset_yaml_path),\n",
    "        split='test',\n",
    "        imgsz=config['img_size'],\n",
    "        batch=config['batch_size'],\n",
    "        conf=config['confidence_threshold'],\n",
    "        iou=config['iou_threshold'],\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Display metrics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Evaluation Metrics\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"  mAP@0.5:      {metrics.box.map50:.4f}\")\n",
    "    print(f\"  mAP@0.5:0.95: {metrics.box.map:.4f}\")\n",
    "    print(f\"  Precision:    {metrics.box.mp:.4f}\")\n",
    "    print(f\"  Recall:       {metrics.box.mr:.4f}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Plot confusion matrix if available\n",
    "    if hasattr(metrics, 'confusion_matrix') and metrics.confusion_matrix is not None:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(metrics.confusion_matrix.matrix, annot=True, fmt='g', cmap='Blues')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        cm_path = OUTPUTS_DIR / 'confusion_matrix_custom.png'\n",
    "        plt.savefig(cm_path)\n",
    "        print(f\"\\nConfusion matrix saved to: {cm_path}\")\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"\\nNo test dataset found.\")\n",
    "    print(\"To evaluate the model, add test images and labels:\")\n",
    "    print(f\"  - Images: {IMAGES_DIR / 'test'}\")\n",
    "    print(f\"  - Labels: {LABELS_DIR / 'test'}\")\n",
    "    print(\"\\nTest set evaluation requires labeled ground truth data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b34e19f",
   "metadata": {},
   "source": [
    "## 10. Model Export and Deployment\n",
    "\n",
    "Export the trained model for deployment and run inference demonstrations.\n",
    "\n",
    "**Export Formats Supported:**\n",
    "- **ONNX**: Cross-platform inference\n",
    "\n",
    "This section demonstrates model export and compares inference performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd9a7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if trained model exists\n",
    "best_model_path = MODELS_DIR / 'yolo_dog_custom' / 'weights' / 'best.pt'\n",
    "last_model_path = MODELS_DIR / 'yolo_dog_custom' / 'weights' / 'last.pt'\n",
    "\n",
    "if best_model_path.exists():\n",
    "    print(\"=\"*60)\n",
    "    print(\"Model Export and Deployment\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load the best trained model\n",
    "    print(f\"\\nLoading best trained model from: {best_model_path}\")\n",
    "    export_model = YOLO(str(best_model_path))\n",
    "    print(f\"Model loaded successfully!\")\n",
    "    \n",
    "    # Create exports directory\n",
    "    exports_dir = OUTPUTS_DIR / 'exports'\n",
    "    exports_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(f\"\\nExport directory: {exports_dir}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    print(\"Exporting model to ONNX format...\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Export to ONNX\n",
    "        onnx_path = export_model.export(\n",
    "            format='onnx',\n",
    "            imgsz=config['img_size'],\n",
    "            simplify=True,  # Simplify ONNX model\n",
    "            opset=12  # ONNX opset version\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nModel exported successfully!\")\n",
    "        print(f\"  ONNX model saved to: {onnx_path}\")\n",
    "        \n",
    "        # Get file size\n",
    "        file_size_mb = os.path.getsize(onnx_path) / (1024 * 1024)\n",
    "        print(f\"  File size: {file_size_mb:.2f} MB\")\n",
    "        \n",
    "        print(\"\\nONNX model can be used with:\")\n",
    "        print(\"  - ONNX Runtime (CPU/GPU)\")\n",
    "        print(\"  - TensorRT (NVIDIA)\")\n",
    "        print(\"  - OpenVINO (Intel)\")\n",
    "        print(\"  - Various inference frameworks\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nExport failed: {e}\")\n",
    "        print(\"\\nNote: ONNX export requires 'onnx' package:\")\n",
    "        print(\"  pip install onnx\")\n",
    "else:\n",
    "    print(\"No trained model found!\")\n",
    "    print(f\"Expected model at: {best_model_path}\")\n",
    "    print(\"\\nPlease train the model first (Section 6) before exporting.\")\n",
    "    print(\"The model will be saved after training completes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0089a0f5",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates how to train a custom YOLOv8 model on your own dog dataset for specialized detection tasks.\n",
    "\n",
    "### Key Points\n",
    "- Fine-tunes YOLOv8 on custom labeled data\n",
    "- Requires proper dataset preparation in YOLO format\n",
    "- Provides training metrics and evaluation\n",
    "- Supports inference on images and videos with trained model\n",
    "\n",
    "### Dataset Requirements\n",
    "1. **Images**: JPG/PNG format in train/val/test directories\n",
    "2. **Labels**: YOLO format .txt files (one per image)\n",
    "3. **Structure**: Organized in `data/images/` and `data/labels/` subdirectories\n",
    "4. **Format**: `class_id center_x center_y width height` (normalized 0-1)\n",
    "\n",
    "### Training Tips\n",
    "- Start with a smaller model (yolov8n.pt) for faster training\n",
    "- Use GPU for significantly faster training times\n",
    "- Monitor training metrics to avoid overfitting\n",
    "- Adjust batch_size based on available GPU memory\n",
    "- Use data augmentation (built into YOLO training)\n",
    "\n",
    "### Next Steps\n",
    "- Experiment with different model sizes (n, s, m, l, x)\n",
    "- Adjust training hyperparameters for better performance\n",
    "- Collect more diverse training data if accuracy is low\n",
    "- Export model for deployment (ONNX, TensorRT, etc.)\n",
    "\n",
    "### Useful Resources\n",
    "- [Ultralytics YOLO Documentation](https://docs.ultralytics.com/)\n",
    "- [YOLO Training Guide](https://docs.ultralytics.com/modes/train/)\n",
    "- [Dataset Preparation Tips](https://docs.ultralytics.com/datasets/)\n",
    "\n",
    "---\n",
    "\n",
    "**Project**: YOLO Dog Detection - Custom Training  \n",
    "**Date**: 2025  \n",
    "**Model**: YOLOv8 Fine-tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a58e40",
   "metadata": {},
   "source": [
    "## 11. Run Inference with Trained Model\n",
    "\n",
    "Execute inference using your trained custom model on images from a folder or live camera feed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f779f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best trained model\n",
    "best_model_path = MODELS_DIR / 'yolo_dog_custom' / 'weights' / 'best.pt'\n",
    "\n",
    "if not best_model_path.exists():\n",
    "    print(\"No trained model found!\")\n",
    "    print(f\"Expected model at: {best_model_path}\")\n",
    "    print(\"\\nPlease train the model first (Section 6) before running inference.\")\n",
    "else:\n",
    "    print(\"=\"*60)\n",
    "    print(\"Custom Model Inference\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nLoaded model from: {best_model_path}\")\n",
    "    inference_model = YOLO(str(best_model_path))\n",
    "    print(f\"Model ready for inference!\")\n",
    "    \n",
    "    # Choose inference mode\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"Select Inference Mode:\")\n",
    "    print(\"-\"*60)\n",
    "    print(\"\\nOption 1: Process images from a folder\")\n",
    "    print(\"Option 2: Real-time camera detection\")\n",
    "    print(\"\\nTo run inference, uncomment ONE of the options below:\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    # ============================================================\n",
    "    # OPTION 1: Process Images from Folder\n",
    "    # ============================================================\n",
    "    # Uncomment the block below to process images from a folder\n",
    "    \n",
    "    # inference_folder = IMAGES_DIR / 'test'  # Change to your folder path\n",
    "    # output_folder = OUTPUTS_DIR / 'inference_results'\n",
    "    # output_folder.mkdir(exist_ok=True)\n",
    "    # \n",
    "    # # Get all images in folder\n",
    "    # image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp']\n",
    "    # images_to_process = []\n",
    "    # for ext in image_extensions:\n",
    "    #     images_to_process.extend(inference_folder.glob(ext))\n",
    "    # \n",
    "    # if len(images_to_process) == 0:\n",
    "    #     print(f\"\\nNo images found in {inference_folder}\")\n",
    "    #     print(\"Please add images to the folder and try again.\")\n",
    "    # else:\n",
    "    #     print(f\"\\nProcessing {len(images_to_process)} images from: {inference_folder}\")\n",
    "    #     print(f\"Saving results to: {output_folder}\")\n",
    "    #     print(\"-\"*60)\n",
    "    #     \n",
    "    #     for img_path in tqdm(images_to_process, desc=\"Processing images\"):\n",
    "    #         # Run inference\n",
    "    #         results = inference_model(\n",
    "    #             str(img_path),\n",
    "    #             conf=config['confidence_threshold'],\n",
    "    #             iou=config['iou_threshold'],\n",
    "    #             verbose=False\n",
    "    #         )\n",
    "    #         \n",
    "    #         # Save annotated image\n",
    "    #         num_detections = len(results[0].boxes)\n",
    "    #         if num_detections > 0:\n",
    "    #             output_path = output_folder / f\"detected_{img_path.name}\"\n",
    "    #             annotated = results[0].plot()\n",
    "    #             cv2.imwrite(str(output_path), annotated)\n",
    "    #             print(f\"  {img_path.name}: {num_detections} dog(s) detected\")\n",
    "    #         else:\n",
    "    #             # Save even if no detections\n",
    "    #             output_path = output_folder / f\"no_detection_{img_path.name}\"\n",
    "    #             annotated = results[0].plot()\n",
    "    #             cv2.imwrite(str(output_path), annotated)\n",
    "    #     \n",
    "    #     print(f\"\\nProcessing complete! Results saved to: {output_folder}\")\n",
    "    \n",
    "    # ============================================================\n",
    "    # OPTION 2: Real-time Camera Detection\n",
    "    # ============================================================\n",
    "    # Uncomment the block below to use camera for real-time detection\n",
    "    \n",
    "    camera_index = 0  # Usually 0 for default camera, try 1, 2, etc. if not working\n",
    "    \n",
    "    print(f\"\\nStarting camera detection (Camera index: {camera_index})...\")\n",
    "    print(\"Press 'q' to quit, 's' to save current frame\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    cap = cv2.VideoCapture(camera_index)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(f\"\\nError: Could not open camera {camera_index}\")\n",
    "        print(\"Try changing camera_index to 1, 2, etc.\")\n",
    "    else:\n",
    "        # Set camera properties\n",
    "        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "        \n",
    "        frame_count = 0\n",
    "        fps_time = time.time()\n",
    "        fps = 0\n",
    "        \n",
    "        camera_output_dir = OUTPUTS_DIR / 'camera_captures'\n",
    "        camera_output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        print(\"Camera opened successfully! Window should appear...\")\n",
    "        \n",
    "        try:\n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                \n",
    "                if not ret:\n",
    "                    print(\"Failed to grab frame\")\n",
    "                    break\n",
    "                \n",
    "                # Run inference\n",
    "                results = inference_model(\n",
    "                    frame,\n",
    "                    conf=config['confidence_threshold'],\n",
    "                    iou=config['iou_threshold'],\n",
    "                    verbose=False\n",
    "                )\n",
    "                \n",
    "                # Get annotated frame\n",
    "                annotated_frame = results[0].plot()\n",
    "                \n",
    "                # Calculate FPS\n",
    "                frame_count += 1\n",
    "                if frame_count % 30 == 0:\n",
    "                    fps = 30 / (time.time() - fps_time)\n",
    "                    fps_time = time.time()\n",
    "                \n",
    "                # Add FPS and detection count to frame\n",
    "                num_detections = len(results[0].boxes)\n",
    "                cv2.putText(annotated_frame, f\"FPS: {fps:.1f}\", (10, 30), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                cv2.putText(annotated_frame, f\"Dogs: {num_detections}\", (10, 70), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                \n",
    "                # Display frame\n",
    "                cv2.imshow('YOLO Dog Detection - Press Q to quit, S to save', annotated_frame)\n",
    "                \n",
    "                # Handle key presses\n",
    "                key = cv2.waitKey(1) & 0xFF\n",
    "                \n",
    "                if key == ord('q'):\n",
    "                    print(\"\\nQuitting camera detection...\")\n",
    "                    break\n",
    "                elif key == ord('s'):\n",
    "                    # Save current frame\n",
    "                    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                    save_path = camera_output_dir / f\"capture_{timestamp}.jpg\"\n",
    "                    cv2.imwrite(str(save_path), annotated_frame)\n",
    "                    print(f\"Frame saved: {save_path}\")\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nInterrupted by user\")\n",
    "        \n",
    "        finally:\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            print(f\"\\nCamera released. Total frames processed: {frame_count}\")\n",
    "            if camera_output_dir.exists():\n",
    "                saved_frames = list(camera_output_dir.glob('*.jpg'))\n",
    "                if saved_frames:\n",
    "                    print(f\"Saved frames: {len(saved_frames)} in {camera_output_dir}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Instructions:\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"1. Uncomment OPTION 1 block to process images from a folder\")\n",
    "    print(\"2. Uncomment OPTION 2 block to use camera for real-time detection\")\n",
    "    print(\"3. Run this cell after uncommenting your chosen option\")\n",
    "    print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
